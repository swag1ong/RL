{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "continue page 120\n",
    "\n",
    "The TD target is an estimate for both reasons: it samples the expected values $E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_{t} = s] $ and it uses current estimate value $V(S_{t+1})$ instead of\n",
    "the true value $v_{\\pi} (S_{t+1})$. Thus, TD methods combine the sampling of MC with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both MC and DP methods.\n",
    "For TD(0), the value estimate for the current state is updated on the basis of the one sample transition from it to the immediately following state. We refer to TD and MC updates as **sample updates** because they involve looking ahead to a sample successor state or state-action pair using the value of the successor, and the reward along the way to compute a back-up value, and the nupdating the value of the original state accordingly. Sample\n",
    "updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors (ie. we know $p(s\\prime, r | s, a)$ in DP).\n",
    "\n",
    "### TD error\n",
    "\n",
    "Finally, note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \\gamma V(S_{t+1})$. This quantity, called the **TD error**, arises in various forms throughout reinforcement learning:\n",
    "\n",
    "$\\delta_{t} = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually avaliable until one time step later. That is, $\\delta_{t}$ is the error in $V(S_{t})$, available at time $t+1$. Also note that if the array V does not change during the episode (as it does not in MC methods), then the MC error can be written as a sum of TD errors:\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1})$\n",
    "\n",
    "$= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_{T} - V(S_T))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t}(0 - 0)$ (ie. the return from terminal state is 0, the same as the value)\n",
    "\n",
    "$= \\sum^{T-1}_{k=t} \\gamma^{k-t} \\delta_{k}$\n",
    "\n",
    "This identity is not exact if V is updated during generating the episode as in TD(0), but if the step size (ie. $\\gamma$) is small the nit may still hold approximately. Generalization of this identity play an important role in the theory and algorithms of TD learning\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}