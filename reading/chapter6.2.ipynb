{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "continue page 120\n",
    "\n",
    "The TD target is an estimate for both reasons: it samples the expected values $E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_{t} = s] $ and it uses current estimate value $V(S_{t+1})$ instead of\n",
    "the true value $v_{\\pi} (S_{t+1})$. Thus, TD methods combine the sampling of MC with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both MC and DP methods.\n",
    "For TD(0), the value estimate for the current state is updated on the basis of the one sample transition from it to the immediately following state. We refer to TD and MC updates as **sample updates** because they involve looking ahead to a sample successor state or state-action pair using the value of the successor, and the reward along the way to compute a back-up value, and the nupdating the value of the original state accordingly. Sample\n",
    "updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors (ie. we know $p(s\\prime, r | s, a)$ in DP).\n",
    "\n",
    "### TD error\n",
    "\n",
    "Finally, note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \\gamma V(S_{t+1})$. This quantity, called the **TD error**, arises in various forms throughout reinforcement learning:\n",
    "\n",
    "$\\delta_{t} = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually avaliable until one time step later. That is, $\\delta_{t}$ is the error in $V(S_{t})$, available at time $t+1$. Also note that if the array V does not change during the episode (as it does not in MC methods), then the MC error can be written as a sum of TD errors:\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1})$\n",
    "\n",
    "$= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_{T} - V(S_T))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t}(0 - 0)$ (ie. the return from terminal state is 0, the same as the value)\n",
    "\n",
    "$= \\sum^{T-1}_{k=t} \\gamma^{k-t} \\delta_{k}$\n",
    "\n",
    "This identity is not exact if V is updated during generating the episode as in TD(0), but if the step size (ie. $\\gamma$) is small the nit may still hold approximately. Generalization of this identity play an important role in the theory and algorithms of TD learning\n",
    "\n",
    "## Advantage of TD prediction methods\n",
    "\n",
    "TD methods update their estimates based in part on other estimates. They learn a guess from a guess ---- they bootstrap. Is this a good thing to do? What advantages do TD methods have over MC and DP methods? in this section we briefly anticipate some of the answers.\n",
    "\n",
    "Obviously, TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions. The next most obvious advantage of TD methods over MC methods is that they are naturally implemented in an online, fully incremental fashion (Does not require $G_{t+1}$, instead only $V(S_{t+1})$). With MC methods, one must wait until the end of an episode, because only then is the return known, whereas with TD methods on need wait only one time step. Surprisingly, often this turns out to be a critical\n",
    "consideration. Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. Other applications are continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter, some MC methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning ($pi(A_t | S_t) = 0$).\n",
    "\n",
    "But what are the disadvantages of TD methods? Certainly it is convenient to learn one guess from the next, without waiting for an actual outcome, but can we still guarantee convergence to $v_{\\pi}$? Happily, the answer is yes. For any fixed policy $\\pi$, TD(0) has been proved to converge to $v_{\\pi}$, in the mean for a constant step-size parameter if it is sufficiently small, and with probability 1 if the step size parameter decreases according to the usual stochastic approximation conditions ($\\sum_{n=1}^{\\infty} a_n (a) = \\infty$ and $\\sum_{n=1}^{\\infty} a_n^2(a) < \\infty$).\n",
    "Most convergence proofs apply only to the table-based case of the algorithm, but some also apply to the case of general linear function approximation.\n",
    "\n",
    "If both TD and MC methods converge asymptotically to the correct predictions, then a natural next question is \"which gets there first?\" In other words, which method learns faster? which makes the more efficient use of limited data? At the current tie, this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question. In practice, however, TD methods have usually been found to converge faster than constant-$\\alpha$ MC methods on stochastic tasks.\n",
    "\n",
    "## optimality of TD(0)\n",
    "\n",
    "Suppose there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer. Given an approximate value function, V, the increments specified above are computed for every time step t at which a non-terminal state is visited, but the value function is changed only once, by the sum of all the increments (after visit all 10 episodes). Then all the available experience is processed again with the new value function to produce a new overall increment, and so on, until the value function converges. We call this **batch updating** because updates are made only after processing each complete batch of training data.\n",
    "\n",
    "Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\\alpha$, as long as $\\alpha$ is chosen to be sufficiently small. The constant-$\\alpha$ MC method also converges deterministically under the same conditions, but to a different answer. Understanding these two answers will help us understand the difference bewteen the two methods. Under normal updating the methods do not move all the way to their respective batch answers, but in some sense they take steps in these directions.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}