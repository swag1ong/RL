{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Monte Carlo methods can be used to estimate value functions and\n",
    "discovering optimal policies. Unlike DP, they do not assume complete\n",
    "knowledge of the environment. MC methods only require experience, sample\n",
    "sequences of states, actions, and rewards from actual or simulated interaction with an\n",
    "environment. Learning from actual experience is striking because it requires no prior\n",
    "knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning\n",
    "from simulated experience is also powerful. Although a model is required, the model need\n",
    "only generate sample transitions, not the complete probability distributions of all possible\n",
    "transitions that is required for dynamic programming (DP). In surprisingly many cases it\n",
    "is easy to generate experience sampled according to the desired probability distributions,\n",
    "but infeasible to obtain the distributions in explicit form.\n",
    "\n",
    "Monte Carlo methods are ways of solving the reinforcement learning problem based on\n",
    "averaging sample returns. To ensure that well-defined returns are available, here we define\n",
    "Monte Carlo methods only for episodic tasks. That is, we assume experience is divided\n",
    "into episodes, and that all episodes eventually terminate no matter what actions are\n",
    "selected. Only on the completion of an episode are value estimates and policies changed.\n",
    "Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in\n",
    "a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for\n",
    "any estimation method whose operation involves a significant random component. Here\n",
    "we use it specifically for methods based on **averaging complete returns**.\n",
    "\n",
    "We adapt the idea of general policy iteration (GPI)\n",
    "developed in Chapter 4 for DP.Whereas there we computed value functions from knowledge\n",
    "of the MDP, here we learn value functions from sample returns with the MDP. The value\n",
    "functions and corresponding policies still interact to attain optimality in essentially the\n",
    "same way (GPI). As in the DP chapter, first we consider the prediction problem, then policy improvement, and,\n",
    "finally, the control problem and its solution by GPI. Each of these ideas taken from DP\n",
    "is extended to the Monte Carlo case in which only sample experience is available.\n",
    "\n",
    "## Monte Carlo Prediction\n",
    "\n",
    "We begin by considering Monte Carlo methods for learning the state-value function for a\n",
    "given policy. Recall that the value of a state is the expected return—expected cumulative\n",
    "future discounted reward—starting from that state. An obvious way to estimate it from\n",
    "experience, then, **is simply to average the returns observed after visits to that state. As\n",
    "more returns are observed**, the average should converge to the expected value. This idea\n",
    "underlies all Monte Carlo methods.\n",
    "\n",
    "### First-visit MC method\n",
    "\n",
    "Suppose we wish to estimate $v_{\\pi} (s)$, the value of a state s under policy $\\pi$, given a set of episodes (i.e rounds of games) obtained by following $\\pi$\n",
    "and passing through s. Each occurrence of state s in an episode is called **a visit to s**. Of course, s may be visited multiple times in the same episode; Let us call the first time it is visited in an\n",
    "episode the first visit to s, the **first-visit MC method** estimates $v_{\\pi} (s)$ as the average of the returns following first visits to s, whereas the **every-visit MC method** averages the returns following all visits to s.\n",
    "These two  MC methods are very similar but have slightly different theoretical properties. First-visit MC has been most widely studied. Every-visit MC extends more naturally to function approximation and eligibility traces.\n",
    "\n",
    "<img src='first-vivit-MC.png'>\n",
    "\n",
    "Unless $S_t$ appears in $S_0, ...., S_{t-1}$ means we only take the first occurrence of state into account.\n",
    "\n",
    "First-visit MC is shown in procedural form in the box. Every-visit MC would be the\n",
    "same except without the check for St having occurred earlier in the episode.\n",
    "\n",
    "Both first-visit MC and every-visit MC converge to $v_{\\pi} (s)$ as the number of visits to s goes to infinity. This is easy to see by LLN.\n",
    "\n",
    "Sometimes, even when one has complete knowledge of the environment's dynamics, the ability of MC methods to work with sample episodes alone can be a significant advantage -- generating the sample\n",
    "games required by Monte Carlo methods is easy.\n",
    "\n",
    "An important fact about Monte Carlo methods is that the estimates for each\n",
    "state are independent. The estimate for one state does not build upon the estimate\n",
    "of any other state, as is the case in DP. In particular, note that the computational expense of estimating the value of\n",
    "a single state is independent of the number of states. This can make Monte Carlo\n",
    "methods particularly attractive when one requires the value of only one or a subset\n",
    "of states. One can generate many sample episodes starting from the states of interest,\n",
    "averaging returns from only these states, ignoring all others. This is a third advantage\n",
    "Monte Carlo methods can have over DP methods\n",
    "\n",
    "## Monte Carlo Estimation of Action Values\n",
    "\n",
    "If a model is not available, then it is particularly useful to estimate action values (the\n",
    "values of state–action pairs) rather than state values. With a model, state values alone are\n",
    "sufficient to determine a policy; one simply looks ahead one step and chooses whichever\n",
    "action leads to the best combination of reward and next state, as we did in the chapter on\n",
    "DP. Without a model, however, state values alone are not sufficient (p is unknown). One must explicitly\n",
    "estimate the value of each action in order for the values to be useful in suggesting a policy.\n",
    "Thus, one of our primary goals for Monte Carlo methods is to estimate $q_{*}$. To achieve\n",
    "this, we first consider the policy evaluation problem for action values.\n",
    "\n",
    "The policy evaluation problem for action values is to estimate $q_{\\pi} (s, a)$, the expected return when starting in state s, taking action a and thereafter following policy $\\pi$. The\n",
    "Monte Carlo methods for this are essentially the same as just presented for state values,\n",
    "except now we talk about visits to a state–action pair rather than to a state. A state–\n",
    "action pair s, a is said to be visited in an episode if ever the state s is visited and action\n",
    "a is taken in it. The every-visit MC method estimates the value of a state–action pair\n",
    "as the average of the returns that have followed all the visits to it. The first-visit MC\n",
    "method averages the returns following the first time in each episode that the state was\n",
    "visited and the action was selected. These methods converge quadratically, as before, to\n",
    "the true expected values as the number of visits to each state–action pair approaches\n",
    "infinity. The only complication is that many state–action pairs may never be visited. If ⇡ is\n",
    "a deterministic policy, then in following ⇡ one will observe returns only for one of the\n",
    "actions from each state. With no returns to average, the Monte Carlo estimates of the\n",
    "other actions will not improve with experience. This is a serious problem because the\n",
    "purpose of learning action values is to help in choosing among the actions available in\n",
    "each state. To compare alternatives we need to estimate the value of all the actions from\n",
    "each state, not just the one we currently favor.\n",
    "\n",
    "### maintaining exploration and exploring starts\n",
    "\n",
    "This is the general problem of **maintaining exploration**. For policy evaluation to work for action\n",
    "values, we must assure continual exploration. One way to do this is by specifying that\n",
    "the episodes start in a state–action pair, and that every pair has a nonzero probability of\n",
    "being selected as the start. This guarantees that all state–action pairs will be visited an\n",
    "infinite number of times in the limit of an infinite number of episodes. We call this the\n",
    "assumption of exploring starts.\n",
    "\n",
    "The assumption of exploring starts is sometimes useful, but of course it cannot be\n",
    "relied upon in general, particularly when learning directly from actual interaction with an\n",
    "environment. In that case the starting conditions are unlikely to be so helpful. The most\n",
    "common alternative approach to assuring that all state–action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all\n",
    "actions in each state.\n",
    "\n",
    "## Monte Carlo Control\n",
    "\n",
    "We are now ready to consider how Monte Carlo estimation can be used in control, that\n",
    "is, to approximate optimal policies. The overall idea is to proceed according to the same\n",
    "pattern as in the DP, that is, according to the idea of generalized policy iteration (GPI). In GPI, one maintains both an approximate a policy and an approximate value function. The value function is repeatedly\n",
    "altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function These two kinds of changes work against each other to\n",
    "some extent, as each creates a moving target for the other, but\n",
    "together they cause both policy and value function to approach\n",
    "optimality.\n",
    "\n",
    "To begin, let us consider a MC version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy policy $\\pi_0$ and ending with the optimal policy and optimal action-value function.\n",
    "\n",
    "<img src='MC-policy-iteration.png'>\n",
    "\n",
    "Policy evaluation is done exactly as described in the preceding section.\n",
    "Many episodes are experienced, with the approximate action-value function approaching\n",
    "the true function asymptotically. For the moment, let us assume that we do indeed\n",
    "observe an infinite number of episodes and that, in addition, the episodes are generated\n",
    "with **exploring starts**. Under these assumptions, the MC methods will compute each $q_{\\pi_k}$ exactly, for arbitrary $\\pi_k$.\n",
    "\n",
    "Policy improvement is done by making the policy greedy wrt the current value function. In this case, we have an action-value function, and therefore no model is needed to construct the greedy policy (ie we do not need to know p). For any action-value function\n",
    "q, the corresponding greedy policy is the one that, for each $s \\in S$, deterministically chooses an action with maximal action-value:\n",
    "\n",
    "$\\pi_{k+1} (s) = argmax_{a} q_{\\pi_k} (s, a)$\n",
    "\n",
    "Policy improvement then can be done by constructing each $\\pi_{k+1}$ as the greedy policy wrt $q_{\\pi_k}$. The policy improvement theorem then applies to $\\pi_{k}, \\pi_{k+1}$ because, for all $s \\in S$,\n",
    "\n",
    "$q_{\\pi_k} (s, \\pi_{k+1} (s)) = q_{\\pi_k} (s, argmax_{a} q_{pi_k} )s, a) = max_{a} q_{\\pi_{k}} (s, a) \\geq q_{\\pi_k} (s, \\pi_k (s)) \\geq v_{\\pi_k} (s)$\n",
    "\n",
    "As discussed previously, the theorem assures us that each $\\pi_{k+1}$ is uniformly better than $\\pi_{k}$, or just as good as $\\pi_k$, in which case they are both optimal policies. This in turn assures us that the overall process converges to the optimal policy and optimal value function. In this way,\n",
    "MC methods can be used to find optimal oplicies given only sample episodes and no other knowledge of the environment's dynamics.\n",
    "\n",
    "We made two unlikely assumptions above in order to easily obtain this guarantee of\n",
    "convergence for the Monte Carlo method.\n",
    "\n",
    "1. The episodes have exploring starts\n",
    "2. Policy evaluation could be done with an infinite number of episodes\n",
    "\n",
    "To obtain a practical algorithm we will have to remove both assumptions. For now, we focus on removing the second assumption, that policy evaluation operates on an infinite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as\n",
    "iterative policy evaluation, which also converge only asymptotically to the true value function. In both DP and MC cases, there are two ways to solve the problem. One is to hold firm to the idea of approximating $q_{\\pi_k}$ in each policy evaluation. Measurements and assumptions are made to obtain bounds\n",
    "on the magnitude and probability of error in the estimates, and then sufficient steps are\n",
    "taken during each policy evaluation to assure that these bounds are sufficiently small.\n",
    "This approach can probably be made completely satisfactory in the sense of guaranteeing\n",
    "correct convergence up to some level of approximation. However, it is also likely to require far too many episodes to be useful in practice on any but the smallest problems.\n",
    "\n",
    "### MCES\n",
    "\n",
    "There is a second approach to avoiding the infinite number of episodes nominally\n",
    "required for policy evaluation, in which we give up trying to complete policy evaluation\n",
    "before returning to policy improvement. On each evaluation step we move the value\n",
    "function toward q⇡k , but we do not expect to actually get close except over many steps.\n",
    "We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme\n",
    "form of the idea is value iteration, in which only one iteration of iterative policy evaluation\n",
    "is performed between each step of policy improvement. The in-place version of value\n",
    "iteration is even more extreme; there we alternate between improvement and evaluation\n",
    "steps for single states.\n",
    "\n",
    "For Monte Carlo policy iteration it is natural to alternate between evaluation and\n",
    "improvement on an episode-by-episode basis. After each episode, the observed returns\n",
    "are used for policy evaluation, and then the policy is improved at all the states visited in\n",
    "the episode. A complete simple algorithm along these lines, which we call Monte Carlo\n",
    "ES, for Monte Carlo with Exploring Starts\n",
    "\n",
    "<img src='MCES.png'>\n",
    "\n",
    "In MCES, all the returns for each state-action pair are accumulated and averaged (For all episodes), irrespective of what policy was in force when they were observed (because $Q(S_t, A_t) \\leftarrow average(Returns(S_t, A_t))$, we are updating the global q(s, a)). It is easy to see that MCES cannot converge to any suboptimal policy.\n",
    "If it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. Stability is achieved only when both the policy and the value function are optimal. Convergence to this optimal fixed point seems inevitable as the changes to the\n",
    "action-value function decrease over time, but has not yet been formally proved.\n",
    "\n",
    "### Monte Carlo Control without Exploring Starts\n",
    "\n",
    "How can we avoid the unlikely assumption of exploring starts? The only general way to\n",
    "ensure that all actions are selected infinitely often is for the agent to continue to select\n",
    "them. There are two approaches to ensuring this, resulting in what we call **on-policy**\n",
    "methods and **off-policy** methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a\n",
    "policy different from that used to generate the data. MCES method is an example of an on-policy method.\n",
    "\n",
    "#### $\\epsilon$-soft policies\n",
    "\n",
    "In on-policy control methods, the policy is generally soft, meaning that $\\pi(a|s) \\geq 0, \\forall s\\in S, a \\in A(s)$, but gradually shifted closer and closer to a deterministic optimal policy. In this section, the on-policy method we present uses $\\epsilon$-greedy policies, meaning that most of the time, they choose an action that has maximal estimated action value, but\n",
    "with some probability $\\epsilon$ they instead select an action at random (Explore). That is, all nongreedy actions are given the minimal probability of selection, $\\frac{\\epsilon}{\\|A(s)\\|}$, and the remaining bulk of the probability, $1 - \\epsilon + \\frac{\\epsilon}{\\|A(s)\\|}$ is given to the greedy action (because random selection of actions can select the optimal action).\n",
    "The $\\epsilon$-greedy policies are examples of $\\epsilon$-soft policies, defined as policies for which $\\pi(a|s) \\geq \\frac{\\epsilon}{\\|A(s)\\|}$ for all states and actions, for some $\\epsilon > 0$. Among $\\epsilon$-soft policies, $\\epsilon$-greedy policies are in some sense those that are closest to greedy.\n",
    "\n",
    "The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte\n",
    "Carlo ES, we use first-visit MC methods to estimate the action-value function for the\n",
    "current policy. Without the assumption of exploring starts, however, we cannot simply\n",
    "improve the policy by making it greedy with respect to the current value function, because\n",
    "that would prevent further exploration of nongreedy actions. Fortunately, GPI does not\n",
    "require that the policy be taken all the way to a greedy policy, only that it be moved\n",
    "toward a greedy policy. In our on-policy method we will move it only to an $\\epsilon$-greedy policy. For any $\\epsilon$-soft policy, $\\pi$, any $\\epsilon$-greedy policy wrt $q_{\\pi}$ is guaranteed to be better than or equal to $\\pi$\n",
    "\n",
    "<img src='epsilon-soft-policies.png'>\n",
    "\n",
    "That any $\\epsilon$-greedy policy wrt $q_{\\pi}$ is an improvement over any $\\epsilon$-soft policy $\\pi$ is assured by the policy improvement theorem. Let $\\pi^\\prime$ be the $\\epsilon$-greedy policy. The conditions of the policy improvement theorem apply because for any $s \\in S$:\n",
    "\n",
    "$q_{\\pi} (s, \\pi^\\prime(s)) = \\sum_{a} \\pi^{\\prime}(a|s) q_{\\pi}(s, a)$ (This is the expected value of $q_{\\pi} (s, \\pi^\\prime (s))$ where $\\pi^\\prime (s)$ is a probability distribution)\n",
    "\n",
    "$= \\frac{\\epsilon}{\\|A(s)\\|} \\sum_{a} q_{\\pi} (s, a) + (1 - \\epsilon) max_{a} q_{\\pi} (s, a)$\n",
    "\n",
    "Since $\\pi (a | s) = \\frac{\\epsilon}{\\|A(s)\\|}$ if $a \\neq argmax_{a} q_{\\pi} (s, a)$\n",
    "\n",
    "\\begin{equation}\n",
    "\\implies \\frac{\\pi(a | s) - \\frac{\\epsilon}{\\|A(s)\\|}}{1 - \\epsilon} =\n",
    "\\begin{cases}\n",
    "  1 &  a = argmax_{a} q_{\\pi} (s, a)\\\\\n",
    "  0, & o/w\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$\\implies \\frac{\\epsilon}{\\|A(s)\\|} \\sum_{a} q_{\\pi} (s, a) + (1 - \\epsilon) max_{a} q_{\\pi} (s, a) \\geq \\frac{\\epsilon}{\\|A(s)\\|} \\sum_{a} q_{\\pi} (s, a) + (1 - \\epsilon) \\sum_{a} \\frac{\\pi(a | s) - \\frac{\\epsilon}{\\|A(s)\\|}}{1 - \\epsilon} q_{\\pi} (s, a) $\n",
    "\n",
    "$ = \\frac{\\epsilon}{\\|A(s)\\|} \\sum_{a} q_{\\pi} (s, a) + \\sum_{a} \\pi(a | s) q_{\\pi} (s, a) - \\sum_{a} \\frac{\\epsilon}{\\|A(s)\\|} q_{\\pi} (s, a) $\n",
    "\n",
    "$ = v_{\\pi} (s)$\n",
    "\n",
    "Thus, by policy improvement theorem, $\\pi^\\prime \\geq \\pi$ (ie. $v_{\\pi^\\prime} (s) \\geq v_{\\pi} (s), \\forall s \\in S$).\n",
    "\n",
    "We now can prove that equality can hold only when both $\\pi^\\prime$ and $\\pi$ are optimal among the $\\epsilon$-soft policies, that is, when they are better than or equal to all other\n",
    "$\\epsilon$-soft policies:\n",
    "\n",
    "<img src='prove_1.png'>\n",
    "\n",
    "In essence, we have shown in the last few pages that policy iteration works for \"-soft\n",
    "policies. Using the natural notion of greedy policy for \"-soft policies, one is assured of\n",
    "improvement on every step, except when the best policy has been found among the \"-soft\n",
    "policies. This analysis is independent of how the action-value functions are determined at each stage, but it does assume that they are computed exactly. This brings us to\n",
    "roughly the same point as in the previous section. Now we only achieve the best policy\n",
    "among the \"$\\epsilon$-soft policies, but on the other hand, we have eliminated the assumption of\n",
    "exploring starts.\n",
    "\n",
    "#### Off-policy Prediction via Importance Sampling\n",
    "\n",
    "All learning control methods face a dilemma: They seek to learn action values conditional\n",
    "on subsequent optimal behavior, but they need to behave non-optimally in order to\n",
    "explore all actions (to find the optimal actions). How can they learn about the optimal\n",
    "policy while behaving according to an exploratory policy? The on-policy approach in the\n",
    "preceding section is actually a compromise—it learns action values not for the optimal\n",
    "policy, but for a near-optimal policy that still explores. More straightforward approach\n",
    "is to use two policies, one that is learned about and that becomes the optimal policy, and\n",
    "one that is more exploratory and is used to generate behavior. The policy being learned\n",
    "about is called the **target policy**, and the policy used to generate behavior is called the\n",
    "**behavior policy**. In this case we say that learning is from data “off” the target policy, and\n",
    "the overall process is termed **off-policy learning**.\n",
    "\n",
    "On-policy methods are generally simpler and are considered first. O↵-policy methods\n",
    "require additional concepts and notation, and because the data is due to a di↵erent policy,\n",
    "o↵-policy methods are often of greater variance and are slower to converge. On the other\n",
    "hand, o↵-policy methods are more powerful and general. They include on-policy methods\n",
    "as the special case in which the target and behavior policies are the same.\n",
    "\n",
    "We begin the study of off-policy methods by considering the prediction problem, in which both target and behavior policies are fixed. That is,\n",
    "suppose we wish to estimate $v_{\\pi}$ or $q_{\\pi}$, but all we have are episodes following another policy b, where $b \\neq \\pi$. In this case, $\\pi$ is the target policy, b is the behavior policy, and both policies are considered fixed and given.\n",
    "\n",
    "In order to use episodes from b to estimate values for $\\pi$, we require that every action taken under $\\pi$ is also taken, at least occasionally under b. That is, we require that $\\pi(a|s) > 0 \\implies b(a|s) > 0$. This is called **the assumption of coverage**.\n",
    "It follows from coverage that b must be stochastic in states where it is not identical to $\\pi$. The target policy $\\pi$, on the other hand, may be deterministic, and, in fact, this is a case of particular interest in control applications. In control, the target policy is typically the\n",
    "deterministic greedy policy wrt the current estimate of the action-value function. This policy becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an $\\epsilon$-greedy policy. However, in this section, we consider the prediction problem (ie. estimate value functions given policy), in which $\\pi$ is unchanging and given.\n",
    "\n",
    "##### Importance sampling\n",
    "\n",
    "Almost all off-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies called the **importance-sampling rato**.\n",
    "Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A_t, S_{t+1}, A_{t+1}, ..., S_{T}$, occuring under any policy $\\pi$ is\n",
    "\n",
    "$P(A_t, S_{t+1}, A_{t+1}, ..., S_{T} | S_t), \\forall A_{i} \\sim \\pi$\n",
    "\n",
    "$= p(A_t | S_t) p(S_{t+1} | S_t, A_t) .... p(S_T | S_{T-1}. A_{T-1})$ (ie. Chain rule + memoryless property)\n",
    "\n",
    "$= \\pi(A_t | S_t)p(S_{t+1} | S_t, A_t)....p(S_T | S_{T-1}. A_{T-1})$\n",
    "\n",
    "$= \\prod^{T-1}_{k=t} \\pi(A_k|S_k) p(S_{k+1} | S_k, A_k)$\n",
    "\n",
    "Thus, the relative probability of the trajectory under the target and behavior policies (The importance-sampling ratio) is:\n",
    "\n",
    "$\\rho_{t:T-1} = \\frac{\\prod^{T-1}_{k=t} \\pi(A_k|S_k) p(S_{k+1} | S_k, A_k)}{\\prod^{T-1}_{k=t} b(A_k|S_k) p(S_{k+1} | S_k, A_k)} = \\frac{\\prod^{T-1}_{k=t} \\pi(A_k|S_k)} {\\prod^{T-1}_{k=t} b(A_k|S_k)}$\n",
    "\n",
    "is the ratio between likelihood of sequence of actions from $\\pi$ and likelihood of same sequence of actions from b.\n",
    "\n",
    "Recall that we wish to estimate $v_{\\pi}, q_{\\pi}$, but all we have is $v_{b}, q_{b}$. These returns have wrong expectations and so cannot be averaged to obtain $v_{\\pi}$ (i.e $\\pi, b$ are two different distribution). This is where importance sampling comes in.\n",
    "The ratio $\\rho_{t:T-1}$ transforms the returns to have the right expected value:\n",
    "\n",
    "$v_{\\pi} (s) = E[\\rho_{t:T-1} G_t | S_t=s]$, where $G_t$ is estimated following b. If $\\rho_{t:T-1} < 1 \\implies$ the sequence is more likely to be in b, so it should have less weight.\n",
    "\n",
    "Now we are ready to give a Monte Carlo algorithm that averages returns from a batch of observed episodes following policy b to estimate $v_{\\pi}$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time t=101. This enables us to use time-step\n",
    "numbers to refer to particular steps in particular episodes. in particular, we can define the set of all time setps in which state s is visited, denoted J(s), this is for an every-visit method; for a first-visit method, J(s) would only include time steps that were first visits to s within their episodes. Also, let T(t) denotes the first time of termination following time t (if t=50, termination after t=50 is 60, then T(50) = 60), and $G_t$ denote the return after t up through T(t). Then $\\{G_{t}\\}_{t \\in J(s)}$ are the returns that pertain to stat s, and\n",
    "$\\{\\rho_{t:T(t)-1}\\}_{t \\in J(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\\pi} (s)$, we simply scale the return by the ratios and average the results:\n",
    "\n",
    "$v_{\\pi}(s) = \\frac{\\sum_{t \\in J(s)} \\rho_{t:T(t)-1} G_t}{\\|J(s)\\|}$ (estimation of expected value by LLN)\n",
    "\n",
    "When importance sampling is done as a simple average in this way it is called **oridinary importance sampling**, an importance alternative is **weighted importance sampling** which uses a weighted average defined as:\n",
    "\n",
    "$v_{\\pi}(s) = \\frac{\\sum_{t \\in J(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in J(s)} \\rho_{t:T(t)-1}}$\n",
    "\n",
    "or zero if the denominator is zero. To understand these two varieties of importance sampling, consider the estimates of their first-visit methods after observing a single return from state s. In the\n",
    "weighted-average estimate, the ratio $\\rho_{t:T(t)-1}$ cancels in the numerator and denominator for *a single return*, so that the estimate is equal to the observed return independent of the ratio. Given\n",
    "that this return was the only one observed, that is a reasonable estimate, but its expectation is $v_{b} (s)$ rather than $v_{\\pi} (s)$, and in this statistical sense, it is biased. in contrast, the first-visit version of the ordinary importance-sampling estimator is always\n",
    "$v_{\\pi} (s)$ in expectation, but it can be extreme. Suppose the ratio were ten, indicating that the trajectory observed is ten times as likely under the target policy as under the behavior policy. in this case, the oridinary importance-sampling estimate would be ten times the observed return. That is, it would be quite far from the observed return even though the episode's trajectory is\n",
    "considered very representative of the target policy.\n",
    "\n",
    "##### Bias Variance tradeoff between OIS and WIS\n",
    "\n",
    "Formally, the difference between the first-visit methods of the two kinds of importance\n",
    "sampling is expressed in their biases and variances. Ordinary importance sampling is\n",
    "unbiased whereas weighted importance sampling is biased when sample size is small (the bias converges\n",
    "asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios\n",
    "can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite.\n",
    "In practice, the weighted estimator usually has dramatically lower variance and is trongly preferred. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation.\n",
    "\n",
    "The every-visit methods for ordinary and weighed importance sampling are both biased,\n",
    "though, again, the bias falls asymptotically to zero as the number of samples increases.\n",
    "In practice, every-visit methods are often preferred because they remove the need to keep\n",
    "track of which states have been visited and because they are much easier to extend to\n",
    "approximations.\n",
    "\n",
    "Below is the off-policy every-visit MC algorithm using weighted importance sampling\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Monte Carlo prediction methods can be implemented incrementally, on an episode by episode\n",
    "basis, For off policy Monte Carlo methods, we need to separately\n",
    "consider those that use ordinary importance sampling and those that use weighted\n",
    "importance sampling. In ordinary importance sampling, the returns are scaled by the importance sampling ratio $\\rho_{t:T(t)-1}$, then simply averaged. For these methods we can us the incremental methods, but using the scaled returns in place of the rewards (Chapter 2). This leaves the case of off-policy methods using weighted importance sampling. Here\n",
    "we have to form a weighted average of the returns, and a slightly different incremental algorithm is required.\n",
    "\n",
    "Suppose we have a sequence of returns $G_1, ...., G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$ (i.e $W_i = \\rho_{t_i:T(t_i)-1}$). We wish to form the estimate\n",
    "\n",
    "$V_{n} = \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k}, n\n",
    "\\geq 2$\n",
    "\n",
    "and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first n returns ($\\sum_{k=1}^{n-1} W_k$). The update rule for $V_n$ is:\n",
    "\n",
    "$V_{n+1} = \\frac{\\sum_{k=1}^{n} W_k G_k}{\\sum_{k=1}^{n} W_k} = \\frac{\\sum_{k=1}^{n-1} W_k G_k}{C_n} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= \\frac{(C_n - W_n)\\sum_{k=1}^{n-1} W_k G_k}{C_n (C_n - W_n)} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= \\frac{C_n\\sum_{k=1}^{n-1} W_k G_k}{C_n (C_n - W_n)} - \\frac{W_n\\sum_{k=1}^{n-1} W_k G_k}{C_n (C_n - W_n)} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= \\frac{\\sum_{k=1}^{n-1} W_k G_k}{C_n - W_n} - \\frac{W_n\\sum_{k=1}^{n-1} W_k G_k}{C_n (C_n - W_n)} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= \\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k} - \\frac{W_n\\sum_{k=1}^{n-1} W_k G_k}{C_n (\\sum_{k=1}^{n-1} W_k)} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= V_n - \\frac{W_n}{C_n}\\frac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k} + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$= V_n - \\frac{W_n}{C_n}V_n + \\frac{W_n G_n}{C_n}$\n",
    "\n",
    "$\\implies V_{n+1} = V_n + \\frac{W_n}{C_n} [G_n - V_n], n \\geq 1$\n",
    "\n",
    "$C_{n+1} = C_n + W_{n+1}, C_0 = 0$\n",
    "\n",
    "The algorithm is nominally for the off-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same ($\\pi = b$)\n",
    "\n",
    "<img src='off-policy-WIS.png'>\n",
    "\n",
    "#### Off-policy MC Control\n",
    "\n",
    "We are now ready to present an example of the second class of learning control methods\n",
    "we consider in this book: off-policy methods. Recall that the distinguishing feature of\n",
    "on-policy methods is that they estimate the value of a policy while using it for control.\n",
    "In off-policy methods these two functions are separated. An advantage of this separation is\n",
    "that the target policy may be deterministic (e.g., greedy), while the behavior policy can\n",
    "continue to sample all possible actions.\n",
    "\n",
    "Off-policy Monte Carlo control methods use one of the techniques presented in the\n",
    "preceding two sections. They follow the behavior policy while learning about and\n",
    "improving the target policy. These techniques require that the behavior policy has a\n",
    "nonzero probability of selecting all actions that might be selected by the target policy\n",
    "(coverage). To explore all possibilities, we require that the behavior policy be soft (i.e.,\n",
    "that it select all actions in all states with nonzero probability).\n",
    "\n",
    "<img src='off-policy-mc-control.png'>\n",
    "\n",
    "The reason $W = W\\frac{1}{b(A_t | S_t)}$ is that when $A_t = \\pi(S_t)\\implies \\pi(A_t | S_t) = 1$\n",
    "\n",
    "A potential problem is that this method learns only from the tails of episodes, when\n",
    "all of the remaining actions in the episode are greedy. If nongreedy actions are common,\n",
    "then learning will be slow, particularly for states appearing in the early portions of\n",
    "long episodes. Potentially, this could greatly slow learning. There has been insufficient\n",
    "experience with o↵-policy Monte Carlo methods to assess how serious this problem is. If\n",
    "it is serious, the most important way to address it is probably by incorporating temporal\n",
    "difference learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, actions, num_eps, env, init_value=0.1, gamma=0.5):\n",
    "\n",
    "        self.Q = {}\n",
    "        self.C = {}\n",
    "        self.pi = {}\n",
    "        self.b = {}\n",
    "        self.num_eps = num_eps\n",
    "        self.actions = actions\n",
    "        self.init_value = init_value\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        curr_ep = 0\n",
    "\n",
    "        while curr_ep < self.num_eps:\n",
    "            trajectory = self._generate_episode()\n",
    "            G = 0\n",
    "            W = 1\n",
    "\n",
    "            for t in range(len(trajectory['s']) - 1, -1, -1):\n",
    "                s_t = trajectory['s'][t]\n",
    "                r_t = trajectory['r'][t]\n",
    "                a_t = trajectory['a'][t]\n",
    "\n",
    "                # get index of action in actions list\n",
    "                a_t = self.actions.index(a_t)\n",
    "                G = self.gamma * G + r_t\n",
    "                self.C[s_t][a_t] = self.C[s_t][a_t] + W\n",
    "                self.Q[s_t][a_t] = self.Q[s_t][a_t] + W * (G - self.Q[s_t][a_t]) / self.C[s_t][a_t]\n",
    "                self.pi[s_t] = np.argmax(self.Q[s_t])\n",
    "\n",
    "                if a_t != self.pi:\n",
    "                    break\n",
    "\n",
    "                W = W / self.b[s_t][a_t]\n",
    "\n",
    "            curr_ep += 1\n",
    "\n",
    "            print(f'episode: {curr_ep} finished')\n",
    "            print(f'total reward is {G}')\n",
    "\n",
    "        return self.pi\n",
    "\n",
    "    def _init_states(self, s):\n",
    "\n",
    "        p = [1 / len(self.actions)] * len(self.actions)\n",
    "\n",
    "        if s not in self.b.keys():\n",
    "            self.b[s] = p\n",
    "\n",
    "        if s not in self.Q.keys():\n",
    "            self.Q[s] = [self.init_value] * len(self.actions)\n",
    "            self.C[s] = [self.init_value] * len(self.actions)\n",
    "            self.pi[s] = np.random.choice(range(len(self.actions)), p=p)\n",
    "\n",
    "    def _generate_episode(self):\n",
    "\n",
    "        curr_state = tuple(self.env.reset())\n",
    "        stop = False\n",
    "        trajectory = {'s': [curr_state],\n",
    "                      'a': [],\n",
    "                      'r': []}\n",
    "\n",
    "        while not stop:\n",
    "\n",
    "            self._init_states(curr_state)\n",
    "\n",
    "            action = np.random.choice(self.actions, p=self.b[curr_state])\n",
    "            curr_state, reward, stop, _ = env.step(action)\n",
    "            curr_state = tuple(curr_state)\n",
    "\n",
    "            if not stop:\n",
    "                trajectory['s'].append(curr_state)\n",
    "\n",
    "            trajectory['r'].append(reward)\n",
    "            trajectory['a'].append(action)\n",
    "\n",
    "        return trajectory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "agent = Agent([0, 1], 20000, env, 0.1, 0.5)\n",
    "best_policy = agent.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}