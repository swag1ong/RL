{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Temporal-Difference Learning\n",
    "\n",
    "## TD Prediction\n",
    "\n",
    "Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following $\\pi$, we estimate $V_{\n",
    "\\pi} (S_t)$ occurring in that experience. Roughly speaking, MC methods wait until the return following the visit is known, then use that return as\n",
    "a target for $V(S_t)$. A simple every visit MC method suitable for non-stationary environments is:\n",
    "\n",
    "$V(S_t) = V(S_t) + \\alpha (G_t - V(S_t))$\n",
    "\n",
    "Where $G_t$ is the actual reward after time t, and $\\alpha$ is a constant step size parameter. Let's call this constant-$\\alpha$ MC. Whereas MC methods must wait until the end of the\n",
    "episode to determine the increment to V(S_t) (only then $G_t$ is known, $G_t = R_{t+1} + \\gamma G_{t+1}$). In contrast, TD methods need to wait only until the next time step. At time t + 1 they\n",
    "immediately form a target and make a useful update using the observed reward $R_{t+1}$ and $V(S_{t+1})$.\n",
    "The simplest TD method makes the update:\n",
    "\n",
    "$V(S_t) = V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))$\n",
    "\n",
    "In fact, the target update for MC method is $G_{t}$, whereas the target for the TD update is $R_{t+1} + \\gamma V(S_{t+1})$. This TD method is called\n",
    "**TD(0)**, or one-step TD, because it is a special case of the **TD($\\lambda$)** and n-step TD methods.\n",
    "\n",
    "<img src=\"pngs/tabular-TD0.png\">\n",
    "\n",
    "Because TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method, like DP. We know that:\n",
    "\n",
    "$v_{\\pi} (s) = E_{\\pi} [G_t | S_t = s] = E_{\\pi} [R_{t+1} + \\gamma G_{t+1} | S_{t} = s] =  E_{\\pi} [R_{t+1} | S_{t}] + \\gamma E_{\\pi} [G_{t+1} | S_{t}] = E_{\\pi} [R_{t+1} | S_{t}] + \\gamma E_{\\pi} [E_{\\pi} [G_{t+1} | S_{t+1} = s^\\prime] | S_{t}] = E_{\\pi} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1}) | S_{t} = s]$,\n",
    "\n",
    "MC methods use estimate of $E_{\\pi} [G_t | S_t = s]$ as a target, whereas DP methods use estimate of $E_{\\pi} [R_{t+1} + \\gamma V_{\\pi} (S_{t+1}) | S_{t} = s]$ as a target. The MC target is an estimate because the expected value is not known. (a sample return is used in place of the real expected return)\n",
    "The DP target is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because $v_{\\pi} (S_{t+1})$ is unknown and we use $V_{\\pi}(S_{t+1})$ as an estimate. The TD target is an estimate for both reasons: it samples the expected values $E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_{t} = s] $ and it uses current estimate value $V(S_{t+1})$ instead of\n",
    "the true value $v_{\\pi} (S_{t+1})$. Thus, TD methods combine the sampling of MC with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both MC and DP methods.\n",
    "For TD(0), the value estimate for the current state is updated on the basis of the one sample transition from it to the immediately following state. We refer to TD and MC updates as **sample updates** because they involve looking ahead to a sample successor state or state-action pair using the value of the successor, and the reward along the way to compute a back-up value, and the nupdating the value of the original state accordingly. Sample\n",
    "updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors (ie. we know $p(s\\prime, r | s, a)$ in DP).\n",
    "\n",
    "### TD error\n",
    "\n",
    "Finally, note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \\gamma V(S_{t+1})$. This quantity, called the **TD error**, arises in various forms throughout reinforcement learning:\n",
    "\n",
    "$\\delta_{t} = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
    "\n",
    "Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually avaliable until one time step later. That is, $\\delta_{t}$ is the error in $V(S_{t})$, available at time $t+1$. Also note that if the array V does not change during the episode (as it does not in MC methods), then the MC error can be written as a sum of TD errors:\n",
    "\n",
    "$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1})$\n",
    "\n",
    "$= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2}))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t} (G_{T} - V(S_T))$\n",
    "\n",
    "$= \\delta_t + \\gamma (\\delta_{t+1}) + \\gamma^2 (G_{t+2} - V(S_{t+2})) + .... + \\gamma^{T-t-1} \\delta_{T-1} + \\gamma^{T-t}(0 - 0)$ (ie. the return from terminal state is 0, the same as the value)\n",
    "\n",
    "$= \\sum^{T-1}_{k=t} \\gamma^{k-t} \\delta_{k}$\n",
    "\n",
    "This identity is not exact if V is updated during generating the episode as in TD(0), but if the step size (ie. $\\gamma$) is small the nit may still hold approximately. Generalization of this identity play an important role in the theory and algorithms of TD learning\n",
    "\n",
    "## Advantage of TD prediction methods\n",
    "\n",
    "TD methods update their estimates based in part on other estimates. They learn a guess from a guess ---- they bootstrap. Is this a good thing to do? What advantages do TD methods have over MC and DP methods? in this section we briefly anticipate some of the answers.\n",
    "\n",
    "Obviously, TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions. The next most obvious advantage of TD methods over MC methods is that they are naturally implemented in an online, fully incremental fashion (Does not require $G_{t+1}$, instead only $V(S_{t+1})$). With MC methods, one must wait until the end of an episode, because only then is the return known, whereas with TD methods on need wait only one time step. Surprisingly, often this turns out to be a critical\n",
    "consideration. Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. Other applications are continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter, some MC methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning ($pi(A_t | S_t) = 0$).\n",
    "\n",
    "But what are the disadvantages of TD methods? Certainly it is convenient to learn one guess from the next, without waiting for an actual outcome, but can we still guarantee convergence to $v_{\\pi}$? Happily, the answer is yes. For any fixed policy $\\pi$, TD(0) has been proved to converge to $v_{\\pi}$, in the mean for a constant step-size parameter if it is sufficiently small, and with probability 1 if the step size parameter decreases according to the usual stochastic approximation conditions ($\\sum_{n=1}^{\\infty} a_n (a) = \\infty$ and $\\sum_{n=1}^{\\infty} a_n^2(a) < \\infty$).\n",
    "Most convergence proofs apply only to the table-based case of the algorithm, but some also apply to the case of general linear function approximation.\n",
    "\n",
    "If both TD and MC methods converge asymptotically to the correct predictions, then a natural next question is \"which gets there first?\" In other words, which method learns faster? which makes the more efficient use of limited data? At the current tie, this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question. In practice, however, TD methods have usually been found to converge faster than constant-$\\alpha$ MC methods on stochastic tasks.\n",
    "\n",
    "## optimality of TD(0)\n",
    "\n",
    "Suppose there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer. Given an approximate value function, V, the increments specified above are computed for every time step t at which a non-terminal state is visited, but the value function is changed only once, by the sum of all the increments (after visit all 10 episodes). Then all the available experience is processed again with the new value function to produce a new overall increment, and so on, until the value function converges. We call this **batch updating** because updates are made only after processing each complete batch of training data.\n",
    "\n",
    "Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, $\\alpha$, as long as $\\alpha$ is chosen to be sufficiently small. The constant-$\\alpha$ MC method also converges deterministically under the same conditions, but to a different answer. Understanding these two answers will help us understand the difference between the two methods. Under normal updating the methods do not move all the way to their respective batch answers, but in some sense they take steps in these directions.\n",
    "\n",
    "## Sarsa: On-policy TD Control\n",
    "\n",
    "We turn now to use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration, only this time using TD methods for the evaluation or prediction part. As with MC methods, we face the need to trade off exploration and exploitation, and again approaches fall into two main classes: on-policy methods and off-policy methods. In this section, we present an on-policy TD control method.\n",
    "\n",
    "The first step is to learn an action-value function rather than a state-value function. In particular, for an on-policy method we must estimate $q_{\\pi} (s, a)$ for the current behavior policy $\\pi$ and for all states s and actions a. This can be done using essentially the same TD method described above for learning $v_{\\pi}$. Recall that an episode consists of an alternating sequence of states and state-action pairs. In the previous section, we considered transitions from state to state and learned the values of states. Now we consider\n",
    "transitions from state-action pair to state-actionpair, and learn the values of state-action pairs. Formally, these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values:\n",
    "\n",
    "$Q(S_t, A_t) = Q(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_{t}, A_{t})]$\n",
    "\n",
    "This update is done after every transition from a non-terminal state $S_{t}$. If $S_{t+1}$ is terminal, then $Q_{S_{t+1}, A_{t+1}} = 0$. This rule uses every element of the quintuple of events, ($S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1}$), that make up a transition from one state-action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm.\n",
    "\n",
    "It is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate $q_{\\pi}$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness with respect to $q_{\\pi}$.\n",
    "\n",
    "The convergence properties of the Sarsa algorithm depend on the nature of the policy's dependence on Q. For example, one could use $\\epsilon -$greedy or $\\epsilon -$soft policies. Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state-action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy.\n",
    "\n",
    "<img src='pngs/sarsa.png'>\n",
    "\n",
    "## Q-learning: off-policy TD control\n",
    "\n",
    "One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm konwn as Q-learning, defined by\n",
    "\n",
    "$Q(S_{t}, A_{t}) = Q(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a) - Q(S_{t}, A_{t})]$\n",
    "\n",
    "In this case, the learned action-value function Q, directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enabled early convergence proofs. The policy still has an effect in that it determines which state-action pairs are visited and updated. However,\n",
    "all that is required for correct convergence is that all pairs continue to be updated. As we observed in MC methods, this is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to $q_{*}$. The Q-learning algorithm is shown below\n",
    "\n",
    "<img src='pngs/Q-learning.png'>\n",
    "\n",
    "## Expected Sarsa\n",
    "\n",
    "Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule\n",
    "\n",
    "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma E_{\\pi} [Q(S_{t+1}, A_{t+1}) | S_{t+1}] - Q(S_t, A_t)]$\n",
    "\n",
    "$= Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma * (\\sum_{a} \\underbrace{\\pi(a|S_{t+1})}_{\\text{probability of getting $Q(S_t, a)$}}Q(S_{t+1}, a)) - Q(S_t, A_t)]$\n",
    "\n",
    "but that otherwise follows the schema of Q-learning. Given the next state, $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectration and accordingly it is called **Expected Sarsa**.\n",
    "\n",
    "Expected Sarsa is more complex computationally than sarsa but, in return, it eliminates the variance due to the random selection of $A_{t+1}$. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does.\n",
    "\n",
    "In some senerios, Expected Sarsa was used on-policy, but in general it\n",
    "might use a policy different from the target policy to generate behavior, in which case\n",
    "it becomes an off-policy algorithm. For example, suppose is the greedy policy while\n",
    "behavior is more exploratory; then Expected Sarsa is exactly Q-learning. In this sense\n",
    "Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa.\n",
    "Except for the small additional computational cost, Expected Sarsa may completely\n",
    "dominate both of the other more-well-known TD control algorithms.\n",
    "\n",
    "The policy that you use in the update step determines which it is. If the update step uses a different weighting for action choices than the policy that actually took the action, then you are using Expected SARSA in an off-policy way.\n",
    "## Why Q-learning is off policy, SARSA is not?\n",
    "\n",
    "The reason that Q-learning is off-policy is that it updates its Q-values using the Q-value of the next state s′ and the greedy action a′. In other words, it estimates the return (total discounted future reward) for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy.\n",
    "\n",
    "The reason that SARSA is on-policy is that it updates its Q-values using the Q-value of the next state s′ and the current policy's action a′′. It estimates the return for state-action pairs assuming the current policy continues to be followed.\n",
    "\n",
    "The distinction disappears if the current policy is a greedy policy. However, such an agent would not be good since it never explores.\n",
    "\n",
    "## Maximization Bias and Double Learning\n",
    "\n",
    "All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, (i.e $a = argmax_{a} Q(s, a)$) which is defined with a max, and in Sarsa the policy is often $\\epislon-greedy$ which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a significant positive bias.\n",
    "To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$, are all zero but whose estimated values, $Q(s, a)$ are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this **maximization bias**.\n",
    "\n",
    "One way to view the problem is that it is due to using the same samples both to determin the maximizing action and to estimate its value. Suppose we divided the samples in two sets and used them to learn two independent estimates, call them $Q_{1} (a)$ and $Q_{2} (a)$, each an estimate of the true value $q(a)$, for all $a \\in A$. We could then use one estimate, say $Q_{1}$ to determine the maximizing action $A^{*} = argmax_{a} Q_{1} (a)$, and the other, $Q_{2}$, to provide the estimate of its value $Q_2(A^{*}) = Q_{2} (argmax_{a} Q_1{a})$. This\n",
    "estimate will then be unbiased in the sense that $E[Q_{2} (A^*)] = q(A^*)$. We can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate $Q_{1} (argmax_{a} Q_{2} (a))$ in the next step. This is the idea of **double learning**. note that although we learn two estimates, only one estimate is updated on each sample. double learning doubles the memory requirements, but does not increases the amount of computation per step.\n",
    "\n",
    "The idea of double learning extends naturally to algorithms for full MDPs. For example, the double learning algorithm analogous to Q-learning, called double Q-learning, divides the time steps in two, perhaps by flipping a coin on each step. If the coin comes up heads, the update is\n",
    "\n",
    "$Q_1(S_t, A_t) = Q_1(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q_{2} (S_{t+1}, argmax_{a} Q_{1} (S_{t+1}, a))) - Q_1(S_t, A_t)]$\n",
    "\n",
    "If the coin comes up tails, then the same update is done with $Q_{1}$ and $Q_{2}$ switched, so that Q_2 is updated. The two approximate value functions are treated completely symmetrically. The behavior policy can use both action-value estimates. For example, an $\\epsilon -$greedy policy for double Q-learning could be based on the average of the two action-value estimates.\n",
    "\n",
    "<img src=\"pngs/double-q-learning.png\">\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}