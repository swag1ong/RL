{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# More on Bellman equations\n",
    "\n",
    "## MDP\n",
    "\n",
    "Define our MDP different from Introduction to RL book:\n",
    "\n",
    "<img src='pngs/mdp.png'>\n",
    "\n",
    "Where $M(X)$ is the space of all probability distributions defined over the state space X\n",
    "\n",
    "So we could describe the process as a sequence of state, actions, and rewards:\n",
    "\n",
    "$X_0, A_0, R_1, X_1, A_1, R_2, ...$\n",
    "\n",
    "A deterministic dynamical system always behave exactly the same given the same starting state and action, that is, they can be described as a transition function f instead of transition distribution $\\rho$\n",
    "\n",
    "$x_{t+1} = f(x, a)$\n",
    "\n",
    "\n",
    "## Policy\n",
    "\n",
    "<img src='pngs/policy_1.png'>\n",
    "<img src='pngs/policy.png'>\n",
    "\n",
    "## Policy induced Transition Kernels\n",
    "\n",
    "<img src='pngs/transition_kernel.png'>\n",
    "\n",
    "## Rewards and Returns\n",
    "\n",
    "Let's define expected reward as\n",
    "\n",
    "$r(x, a) = E[R | X=x, A=a]$, then the expected reward following policy pi is defined as\n",
    "\n",
    "$r^{\\pi} (x) = E_{a \\sim \\pi(a | s)} [R | X=x, A=a]$\n",
    "\n",
    "Define return following policy $\\pi$ as discounted sum of rewards starting from $X_0$:\n",
    "\n",
    "$G^{\\pi} = R_1 + \\gamma R_2 + ... + \\gamma^{T-1} R_{T} = \\sum^{\\infty}_{t=1} \\gamma^{t-1}R_{t}$\n",
    "\n",
    "or any starting at any step $X_{t}$:\n",
    "\n",
    "$G^{\\pi}_{t} = \\sum^{\\infty}_{i=t+1} \\gamma^{i + t - 1}R_{i}$\n",
    "\n",
    "Note that $\\gamma$ is a part of the problem setting, it is usually not a hyperparameter.\n",
    "\n",
    "## Value functions\n",
    "\n",
    "Next we can define value functions as:\n",
    "\n",
    "State value function starting from state x and following policy $\\pi$:\n",
    "\n",
    "$V^{\\pi}(x) = E[G^{\\pi} | X_{0} = x] = E[G^{\\pi}_{t} | X_{t} = x]$ (Markov Property)\n",
    "\n",
    "Action value function following policy $\\pi$ starting from state x and taking action a:\n",
    "\n",
    "$Q^{\\pi} (x, a) = E[G^{\\pi} | X_{0} = x, A_{0} = a] = E[G^{\\pi}_{t} | X_{t} = x, A_{t} = a]$ (Markov Property)\n",
    "\n",
    "## Bellman equations\n",
    "\n",
    "By expanding value functions, we reveal the recursive property of return\n",
    "\n",
    "\\begin{aligned}\n",
    "V^{\\pi} (x) &= E[G^{\\pi}_{t} | X_{t} = x]\\\\\n",
    "& = E[R_{t} + \\gamma G^{\\pi}_{t+1} | X_{t} = x]\\\\\n",
    "& = E_{a \\sim \\pi(a | s)}[R_{t} | X_{t} = x] + \\gamma E[G^{\\pi}_{t+1} | X_{t} = x]\\\\\n",
    "& = r^{\\pi} (x) + \\gamma E[V^{\\pi} (X_{t+1}) | X_{t} = x]\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Where\n",
    "$ E[V^{\\pi} (X_{t+1}) | X_{t} = x] = E[E[G^{\\pi}_{t+1} | X_{t+1} = x^\\prime] | X_{t} = x] = E[G^{\\pi}_{t+1} | X_{t} = x]$\n",
    "\n",
    "This implies that neither side is random, by expanding $E[V^{\\pi} (X_{t+1}) | X_{t} = x]$, we get\n",
    "\n",
    "$E[V^{\\pi} (X_{t+1}) | X_{t} = x] = \\int P(dx^\\prime | x, a) \\pi (a | x) V^{\\pi} (x^\\prime)$\n",
    "\n",
    "Thus, the value function can be expressed as:\n",
    "\n",
    "$V^{\\pi} (x) = r^{\\pi} (x) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a | x) V^{\\pi} (x^\\prime)$\n",
    "\n",
    "This is the **bellman equation** for a policy $\\pi$, this can be interpreted as: The value of following a policy $\\pi$ is the expected immediate reward that the $\\pi$-following agent receives at that state plus the\n",
    "discounted average(expected) value that the agent receives at the next-state.\n",
    "\n",
    "same for action-value function, we have bellman equation:\n",
    "\n",
    "$Q^{\\pi} (x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{\\pi}(x^\\prime) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a^\\prime | x^\\prime) Q^{\\pi} (x^\\prime, a^\\prime)$\n",
    "\n",
    "Compare with value function, we have expected reward base on action a plus discounted average(expected) value that the agent receives at the next-state following $\\pi$ (the choice of action is given at state s in action value function)\n",
    "\n",
    "## Bellman Equations for Optimal Value Functions\n",
    "\n",
    "So it is natural to ask, does the optimal value function of optimal policy $V^{\\pi^*}$ have similar structure? the answer is yes, however, we need to prove this.\n",
    "\n",
    "The proof goes with 3 claims which we will prove later:\n",
    "\n",
    "1.\n",
    "$\\exists$ a value function $V^*$ s.t $\\forall x \\in X$, we have:\n",
    "$V^{*}(x) = max_{a} \\{Q^{*}(x, a)\\} = max_{a} \\{r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{*}(x^\\prime)\\}$.\n",
    "\n",
    "$\\exists$ a value function $Q^*$ s.t $\\forall x, a \\in X, A$, we have:\n",
    "$Q^{*}(x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) max_{a^\\prime} \\{Q^{*} (x^\\prime, a^\\prime)\\}$.\n",
    "\n",
    "These equations are called **bellman optimality equations for the value functions**\n",
    "\n",
    "2.\n",
    "$V^{*}$ is the same as ${V^{\\pi^*}}$, the optimal value function when $\\pi$ is restricted to be within the space of all stationary and non-stationary policies.\n",
    "\n",
    "3.\n",
    "For discounted continuing MDPs, we can always find a stationary policy that is optimal within the space of all stationary and non-stationary policies.\n",
    "\n",
    "In summary, we claim that $V^{*}$ exists and it is unique,  $V^{*} = V^{\\pi^{*}}$. Same for $Q^{*}$.\n",
    "\n",
    "## Bellman Operators\n",
    "\n",
    "In order to prove the claims, we need several concepts:\n",
    "\n",
    "<img src='pngs/bellman-operator.png'>\n",
    "<img src='pngs/bellman-opt-operator.png'>\n",
    "\n",
    "These operators are linear and recall that:\n",
    "\n",
    "$Q^{\\pi} (x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{\\pi}(x^\\prime) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a^\\prime | x^\\prime) Q^{\\pi} (x^\\prime, a^\\prime)$\n",
    "\n",
    "This implies $T^{\\pi} Q^{\\pi} = Q^{\\pi}$, same for state-value function and bellman optimality operators. Thus, we can conclude that:\n",
    "\n",
    "$V^{\\pi} = T^{\\pi} V^{\\pi}$\n",
    "\n",
    "$Q^{\\pi} = T^{\\pi} Q^{\\pi}$\n",
    "\n",
    "$V^{*} = T^{*} V^{*}$\n",
    "\n",
    "$Q^{*} = T^{*} Q^{*}$\n",
    "\n",
    "### Properties of Bellman operators\n",
    "\n",
    "Bellman operators have several important properties. The properties that matters for us the most are:\n",
    "\n",
    "1. Monotonicity\n",
    "2. Contraction\n",
    "\n",
    "#### Monotonicity\n",
    "\n",
    "<img src='pngs/monotonicity.png'>\n",
    "\n",
    "we can easily prove this (only prove $T^{\\pi}$, $T^{*}$ is the same):\n",
    "\n",
    "Assume $V_1(x^{\\prime}) \\geq V_2(x^{\\prime}), \\forall x^{\\prime} \\in X$, we get that for any $x \\in X$\n",
    "\n",
    "$T^{\\pi}V_{1} (x) = r^{\\pi} (x) + \\gamma \\int P^{\\pi}(dx^{\\prime} | x) V_{1} (x^\\prime) \\geq r^{\\pi} (x) + \\gamma \\int P^{\\pi}(dx^{\\prime} | x) V_{2} (x^\\prime) = T^{\\pi}V_2(x)$\n",
    "\n",
    "Thus $T^{\\pi}V_{1} (x) \\geq T^{\\pi}V_{2} (x)$\n",
    "\n",
    "#### Contraction\n",
    "\n",
    "<img src='pngs/contraction_mapping.png'>\n",
    "\n",
    "we can also easily prove that $T^{*}, T^{\\pi}$ are contraction mappings (only prove $T^{\\pi}$, $T^{*}$ is the same):\n",
    "\n",
    "Consider two action-value functions $Q_1, Q_2 \\in B(X\\times A)$. Consider the metric $d(Q_1, Q_2) = \\| Q_1 - Q_2 \\|_{\\infty}$. We show the contraction w.r.t $l-\\infty$ norm.\n",
    "\n",
    "For any $(x, a) \\in X \\times A$, we have:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\lvert T^{\\pi} Q_1 (x, a) - T^{\\pi} Q_2 (x, a) \\rvert &= \\lvert r (x, a) + \\gamma \\int P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime) Q_{1} (x^\\prime, a^{\\prime}) - r (x, a) - \\gamma \\int P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime) Q_{2} (x^\\prime, a^{\\prime}) \\rvert\\\\\n",
    "& = \\gamma \\lvert \\int P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime) [Q_{1} (x^\\prime, a^{\\prime}) -  Q_{2} (x^\\prime, a^{\\prime})] \\rvert\\\\\n",
    "& \\leq \\gamma \\int \\underbrace{P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime)}_{\\text{these are probabilities which $ \\geq $ 0}} \\lvert [Q_{1} (x^\\prime, a^{\\prime}) -  Q_{2} (x^\\prime, a^{\\prime})] \\rvert\\\\\n",
    "& \\leq \\gamma \\int P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime) \\sup_{x, a}\\lvert [Q_{1} (x, a) -  Q_{2} (x, a)] \\rvert\\\\\n",
    "& = \\gamma \\sup_{x, a}\\lvert Q_{1} (x, a) -  Q_{2} (x, a) \\rvert \\underbrace{\\int P(dx^{\\prime} | x, a) \\pi(da^\\prime | x^\\prime)}_{\\text{this is a probability distribution which sums to 1}}\\\\\n",
    "& = \\gamma \\sup_{x, a}\\lvert Q_{1} (x, a) -  Q_{2} (x, a) \\rvert = \\gamma \\|Q_{1} -  Q_{2}\\|_{\\infty}\n",
    "\\end{aligned}\n",
    "\n",
    "This inequality holds for all state action pair, thus, we have showed that bellman operators are contraction mappings\n",
    "\n",
    "<img src='pngs/gamma_contraction.png'>\n",
    "\n",
    "### Consequences of Monotonicity and Contraction\n",
    "\n",
    "#### Uniqueness of fixed points\n",
    "\n",
    "<img src='pngs/fixed_point.png'>\n",
    "<img src='pngs/bfpt.png'>\n",
    "<img src='pngs/unique_fixed_points.png'>\n",
    "\n",
    "Prove of the proposition is also simple (Same for $Q^{\\pi}, Q^{*}$):\n",
    "\n",
    "For any $\\pi$, $T^{\\pi}$ is a $\\gamma-$contraction mapping (same for $T^{*}$), thus, by the Banach fixed point theorem, they have unique fixed point\n",
    "$T^{*} V^{*} = V^{*}, T^{\\pi} V^{\\pi} = V^{\\pi}$. Moreover, the update rule $V_{k+1} \\leftarrow T^{\\pi} V_{k}$ converges, which means $lim_{k \\rightarrow \\infty} \\| V_{k} - V^{\\pi}\\|_{\\infty} = 0$\n",
    "\n",
    "#### Value of the greedy policy of $V^{*}$ is $V^{*}$\n",
    "\n",
    "We know if $T^{\\pi} V^{*} = T^{*}V^{*} \\implies T^{\\pi} V^{*} = V^{*}$ This is also the fix point of $T^{\\pi}$, this implies $V^{\\pi} = V^{*}$.\n",
    "\n",
    "One the other hand, if we know $V^\\pi = V^{*}$, the by applying $T^{\\pi}$ to both side we have $T^{\\pi} V^{\\pi} = T^{\\pi} V^{*} \\implies T^{\\pi} V^{\\pi} = T^{*} V^{*} = T^{\\pi} V^{*}$\n",
    "\n",
    "Thus, we can conclude that $T^{\\pi} V^{*} = T^{*}V^{*}$ iff $V^\\pi = V^{*}$.\n",
    "\n",
    "**To see the connection to greedy policy:**\n",
    "\n",
    "Given $V^{*}$, the greedy policy of $V^{*}$ is defined as:\n",
    "\n",
    "$\\pi_{g} (x; V^{*}) = argmax_{a \\in A} Q^{*}(x, a) = argmax_{a \\in A} r(x, a) + \\gamma \\int P(dx^{\\prime} | x, a) V^{*}(x^{\\prime})$\n",
    "\n",
    "So\n",
    "\n",
    "$T^{\\pi_{g} (x; V^{*})} V^{*} = r(x, \\pi_{g} (x; V^{*})) + \\gamma \\int P(dx^{\\prime} | x, \\pi_{g} (x; V^{*})) V^{*}(x^{\\prime}) = max_{a \\in A} r(x, a) + \\gamma \\int P(dx^{\\prime} | x, a) V^{*}(x^{\\prime})$\n",
    "\n",
    "Compare with $T^{*} V^{*}$:\n",
    "\n",
    "$T^{*} V^{*} =  max_{a \\in A} r(x, a) + \\gamma \\int P(dx^{\\prime} | x, a) V^{*}(x^{\\prime})$\n",
    "\n",
    "Thus, $T^{*} V^{*} = T^{\\pi_{g} (x; V^{*})} V^{*}$.\n",
    "\n",
    "Since $T^{\\pi} V^{*} = T^{*}V^{*}$ iff $V^\\pi = V^{*} \\implies V^{*} = V^{\\pi_{g} (x; V^{*})}$\n",
    "\n",
    "That is, the value of following $\\pi_{g} (x; V^{*})$ is the same as $V^{*}$. This means that if we find $V^{*}$ and its greedy policy, the value of following the greedy policy is $V^{*}$\n",
    "\n",
    "**To find an optimal policy, we can find $V^{*}$ first, then find its greedy policy**\n",
    "\n",
    "following this conclusion:\n",
    "\n",
    "Although we have not yet prove that $V^{*}$ is the optimal value function ($\\pi^{*} = argmax_{\\pi \\in \\prod} V^{\\pi} (x)$, $V^{*} = V^{\\pi^{*}}$ (claim 2)), this is indeed true. (prove later)\n",
    "\n",
    "#### An error bound based on the bellman error\n",
    "\n",
    "If we find a V s.t $V = T^{*} V$, we know that $V = V^{*}$ (same for $T^{\\pi} and Q$), what if $V \\approx T^{*} V$, what can we say about the closeness of $V$ to $V^{*}$?\n",
    "\n",
    "<img src='pngs/br_1.png'>\n",
    "<img src='pngs/br_2.png'>\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "\\begin{aligned}\n",
    "V - V^{*} &= V - T^{*} V + T^{*} V - V^{*}\\\\\n",
    "\\implies \\| V - V^{*} \\|_{\\infty} &= \\| V - T^{*} V + T^{*} V - V^{*} \\|_{\\infty}\\\\\n",
    "&\\leq   \\|V - T^{*} V \\|_{\\infty} + \\|T^{*} V - T^{*}V^{*} \\|_{\\infty} \\\\\n",
    "&\\leq   \\|V - T^{*} V \\|_{\\infty} + \\gamma \\|V - V^{*} \\|_{\\infty} (\\|T^{*} V - T^{*}V^{*} \\|_{\\infty} \\leq \\gamma \\|V - V^{*} \\|_{\\infty}, ie. \\text{contraction property} )\\\\\n",
    "\\implies \\| V - V^{*} \\|_{\\infty} &\\leq \\frac{\\|V - T^{*} V \\|_{\\infty}}{(1 - \\gamma)}\\\\\n",
    "\\implies \\| V - V^{*} \\|_{\\infty} &\\leq \\frac{BE^{*}}{(1 - \\gamma)}\n",
    "\\end{aligned}\n",
    "\n",
    "#### Proof of claim 2: $V^{*} = T^{*} V^{*}$ is the same as $V^{\\pi^{*}}$\n",
    "\n",
    "<img src='pngs/claim_2.png'>\n",
    "\n",
    "##### Proof $V^{*} (x) \\leq sup_{\\pi \\in \\prod} V^{\\pi} (x)$\n",
    "\n",
    "From the error bound with the choice of $V = V^{*}$, we have for any $\\pi \\in \\prod$,\n",
    "\n",
    "$\\| V^{*} - V^{\\pi} \\|_{\\infty} \\leq \\frac{\\| V^{*} - T^{\\pi}V^{*} \\|_{\\infty}}{1 - \\gamma}$\n",
    "\n",
    "Let $\\epsilon > 0$. Choose a policy $\\pi_{\\epsilon}$ s.t:\n",
    "\n",
    "$\\| V^{*} - T^{\\pi_{\\epsilon}} V^{*} \\|_{\\infty} \\leq (1 - \\gamma) \\epsilon$\n",
    "\n",
    "We know that this $\\pi_{\\epsilon}$ exists because we can always pick a $\\pi_{\\epsilon}$ s.t it is the maximizer of $T^{*}V^{*}$ (ie. pick $\\pi_{\\epsilon}$ to be $\\pi_{g} (x; V^{*})$, from above proof we know that $T^{\\pi_{g} (x; V^{*})} V^{*} = T^{*} V^{*}$), then\n",
    "\n",
    "$\\| V^{*} - T^{\\pi_{\\epsilon}} V^{*} \\|_{\\infty} = 0 \\leq (1 - \\gamma) * \\epsilon, \\forall \\epsilon > 0$\n",
    "\n",
    "For any policy $\\pi_{\\epsilon}$ that satisfies above equation:\n",
    "\n",
    "$\\| V^{*} - V^{\\pi_{\\epsilon}} \\|_{\\infty} \\leq \\epsilon$\n",
    "\n",
    "This means that\n",
    "\n",
    "$sup_{x_i} \\{|V^{*} (x_i) - V^{\\pi_{\\epsilon}} (x_i)|\\} \\leq \\epsilon$\n",
    "\n",
    "$\\implies |V^{*} (x_i) - V^{\\pi_{\\epsilon}} (x_i)| \\leq sup_{x_i} \\{|V^{*} (x_i) - V^{\\pi_{\\epsilon}} (x_i)|\\} \\leq \\epsilon, \\forall x_i \\in X$\n",
    "\n",
    "$\\implies V^{*} (x_i) - V^{\\pi_{\\epsilon}} (x_i) \\leq  |V^{*} (x_i) - V^{\\pi_{\\epsilon}} (x_i)| \\leq \\epsilon$\n",
    "\n",
    "$\\implies V^{*} (x_i) \\leq \\epsilon + V^{\\pi_{\\epsilon}} (x_i)$\n",
    "\n",
    "Since $V^{\\pi_{\\epsilon}} (x) \\leq sup_{\\pi \\in \\prod} V^{\\pi} (x)$, as we take $\\epsilon \\rightarrow 0$, we have\n",
    "\n",
    "$V^{*} (x) \\leq lim_{\\epsilon \\rightarrow 0} (epsilon + V^{\\pi_{\\epsilon}} (x)) \\leq lim_{\\epsilon \\rightarrow 0} (epsilon + sup_{\\pi \\in \\prod} V^{\\pi}  (x)) = sup_{\\pi \\in \\prod} V^{\\pi} (x)$\n",
    "\n",
    "**This shows that $V^{*} = T^{*} V^{*}$ is upper bounded by the optimal value function within the space of stationary policies**\n",
    "\n",
    "##### Proof $V^{*} (x) \\geq sup_{\\pi \\in \\prod} V^{\\pi} (x)$\n",
    "\n",
    "Consider any $\\pi \\in \\prod$. By the definition of $T^{\\pi}$ and $T^{*}$, for any $V \\in B(X)$, we have that for any $x \\in X$,\n",
    "\n",
    "$T^{\\pi} V = r^{\\pi}(x) + \\int P(dx^{\\prime} | x, a) \\pi(da | x) V(x^{\\prime})$\n",
    "\n",
    "$= \\int  \\pi(da | x) [P(dx^{\\prime} | x, a) V(x^{\\prime}) + r(x, a)]$\n",
    "\n",
    "$\\leq sup_{a \\in A} \\{r(x, a) + \\int P(dx^{\\prime} | x, a) V(x^{\\prime})\\} = T^{*} V$\n",
    "\n",
    "In particular, with the choice of $V = V^{*}$, we have\n",
    "\n",
    "$T^{\\pi} V^{*} \\leq T^{*} V^{*} = V^{*}$\n",
    "\n",
    "$\\implies T^{\\pi}T^{\\pi} V^{*} \\leq T^{\\pi}V^{*} \\leq V^{*}$ (Monotonicity)\n",
    "\n",
    "$\\implies (T^{\\pi})^{k} V^{*} \\leq V^{*}$ (apply bellman operator k times)\n",
    "\n",
    "as $k \\rightarrow \\infty$, by fix point theorem, we know that $(T^{\\pi})^k V^{*} = V^{\\pi}$:\n",
    "\n",
    "$\\implies V^{\\pi} = lim_{k \\rightarrow \\infty} (T^{\\pi})^k V^{*} \\leq V^{*}, \\forall \\pi in \\prod$\n",
    "\n",
    "$\\implies V^{*} (x) \\geq sup_{\\pi \\in \\prod} V^{\\pi} (x)$\n",
    "\n",
    "**Thus, we can conclude that $V^{*} (x) \\geq sup_{\\pi \\in \\prod} V^{\\pi} (x)$**\n",
    "\n",
    "**Together, we showed that $V^{*}$ is the same as $V^{\\pi^{*}}$, the solution of bellman optimality equation is the optimal value function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}