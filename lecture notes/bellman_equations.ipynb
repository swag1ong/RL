{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# More on Bellman equations\n",
    "\n",
    "## MDP\n",
    "\n",
    "Define our MDP different from Introduction to RL book:\n",
    "\n",
    "<img src='./pngs/mdp.png>\n",
    "\n",
    "Where $M(X)$ is the space of all probability distributions defined over the state space X\n",
    "\n",
    "So we could describe the process as a sequence of state, actions, and rewards:\n",
    "\n",
    "$X_0, A_0, R_1, X_1, A_1, R_2, ...$\n",
    "\n",
    "A deterministic dynamical system always behave exactly the same given the same starting state and action, that is, they can be described as a transition function f instead of transition distribution $\\rho$\n",
    "\n",
    "$x_{t+1} = f(x, a)$\n",
    "\n",
    "\n",
    "## Policy\n",
    "\n",
    "<img src='pngs/policy_1.png'>\n",
    "<img src='pngs/policy.png'>\n",
    "\n",
    "## Policy induced Transition Kernels\n",
    "\n",
    "<img src='pngs/transition_kernel.png'>\n",
    "\n",
    "## Rewards and Returns\n",
    "\n",
    "Let's define expected reward as\n",
    "\n",
    "$r(x, a) = E[R | X=x, A=a]$, then the expected reward following policy pi is defined as\n",
    "\n",
    "$r^{\\pi} (x) = E_{a \\sim \\pi(a | s)} [R | X=x, A=a]$\n",
    "\n",
    "Define return following policy $\\pi$ as discounted sum of rewards starting from $X_0$:\n",
    "\n",
    "$G^{\\pi} = R_1 + \\gamma R_2 + ... + \\gamma^{T-1} R_{T} = \\sum^{\\infty}_{t=1} \\gamma^{t-1}R_{t}$\n",
    "\n",
    "or any starting at any step $X_{t}$:\n",
    "\n",
    "$G^{\\pi}_{t} = \\sum^{\\infty}_{i=t+1} \\gamma^{i + t - 1}R_{i}$\n",
    "\n",
    "Note that $\\gamma$ is a part of the problem setting, it is usually not a hyperparameter.\n",
    "\n",
    "## Value functions\n",
    "\n",
    "Next we can define value functions as:\n",
    "\n",
    "State value function starting from state x and following policy $\\pi$:\n",
    "\n",
    "$V^{\\pi}(x) = E[G^{\\pi} | X_{0} = x] = E[G^{\\pi}_{t} | X_{t} = x]$ (Markov Property)\n",
    "\n",
    "Action value function following policy $\\pi$ starting from state x and taking action a:\n",
    "\n",
    "$Q^{\\pi} (x, a) = E[G^{\\pi} | X_{0} = x, A_{0} = a] = E[G^{\\pi}_{t} | X_{t} = x, A_{t} = a]$ (Markov Property)\n",
    "\n",
    "## Bellman equations\n",
    "\n",
    "By expanding value functions, we reveal the recursive property of return\n",
    "\n",
    "\\begin{aligned}\n",
    "V^{\\pi} (x) &= E[G^{\\pi}_{t} | X_{t} = x]\\\\\n",
    "& = E[R_{t} + \\gamma G^{\\pi}_{t+1} | X_{t} = x]\\\\\n",
    "& = E_{a \\sim \\pi(a | s)}[R_{t} | X_{t} = x] + \\gamma E[G^{\\pi}_{t+1} | X_{t} = x]\\\\\n",
    "& = r^{\\pi} (x) + \\gamma E[V^{\\pi} (X_{t+1}) | X_{t} = x]\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Where\n",
    "$ E[V^{\\pi} (X_{t+1}) | X_{t} = x] = E[E[G^{\\pi}_{t+1} | X_{t+1} = x^\\prime] | X_{t} = x] = E[G^{\\pi}_{t+1} | X_{t} = x]$\n",
    "\n",
    "This implies that neither side is random, by expanding $E[V^{\\pi} (X_{t+1}) | X_{t} = x]$, we get\n",
    "\n",
    "$E[V^{\\pi} (X_{t+1}) | X_{t} = x] = \\int P(dx^\\prime | x, a) \\pi (a | x) V^{\\pi} (x^\\prime)$\n",
    "\n",
    "Thus, the value function can be expressed as:\n",
    "\n",
    "$V^{\\pi} (x) = r^{\\pi} (x) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a | x) V^{\\pi} (x^\\prime)$\n",
    "\n",
    "This is the **bellman equation** for a policy $\\pi$, this can be interpreted as: The value of following a policy $\\pi$ is the expected immediate reward that the $\\pi$-following agent receives at that state plus the\n",
    "discounted average(expected) value that the agent receives at the next-state.\n",
    "\n",
    "same for action-value function, we have bellman equation:\n",
    "\n",
    "$Q^{\\pi} (x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{\\pi}(x^\\prime) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a^\\prime | x^\\prime) Q^{\\pi} (x^\\prime, a^\\prime)$\n",
    "\n",
    "Compare with value function, we have expected reward base on action a plus discounted average(expected) value that the agent receives at the next-state following $\\pi$ (the choice of action is given at state s in action value function)\n",
    "\n",
    "## Bellman Equations for Optimal Value Functions\n",
    "\n",
    "So it is natural to ask, does the optimal value function of optimal policy $V^{\\pi^*}$ have similar structure? the answer is yes, however, we need to prove this.\n",
    "\n",
    "The proof goes with 3 claims which we will prove later:\n",
    "\n",
    "1.\n",
    "$\\exists$ a value function $V^*$ s.t $\\forall x \\in X$, we have:\n",
    "$V^{*}(x) = max_{a} \\{Q^{*}(x, a)\\} = max_{a} \\{r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{*}(x^\\prime)\\}$.\n",
    "\n",
    "$\\exists$ a value function $Q^*$ s.t $\\forall x, a \\in X, A$, we have:\n",
    "$Q^{*}(x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) max_{a^\\prime} \\{Q^{*} (x^\\prime, a^\\prime)\\}$.\n",
    "\n",
    "These equations are called **bellman optimality equations for the value functions**\n",
    "\n",
    "2.\n",
    "$V^{*}$ is the same as ${V^{\\pi^*}}$, the optimal value function when $\\pi$ is restricted to be within the space of all stationary and non-stationary policies.\n",
    "\n",
    "3.\n",
    "For discounted continuing MDPs, we can always find a stationary policy that is optimal within the space of all stationary and non-stationary policies.\n",
    "\n",
    "In summary, we claim that $V^{*}$ exists and it is unique,  $V^{*} = V^{\\pi^{*}}$. Same for $Q^{*}$.\n",
    "\n",
    "## Bellman Operators\n",
    "\n",
    "In order to prove the claims, we need several concepts:\n",
    "\n",
    "<img src='pngs/bellman-operator.png'>\n",
    "<img src='pngs/bellman-opt-operator.png'>\n",
    "\n",
    "These operators are linear and recall that:\n",
    "\n",
    "$Q^{\\pi} (x, a) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) V^{\\pi}(x^\\prime) = r(x, a) + \\gamma \\int P(dx^\\prime | x, a) \\pi (a^\\prime | x^\\prime) Q^{\\pi} (x^\\prime, a^\\prime)$\n",
    "\n",
    "This implies $T^{\\pi} Q^{\\pi} = Q^{\\pi}$, same for state-value function and bellman optimality operators. Thus, we can conclude that:\n",
    "\n",
    "$V^{\\pi} = T^{\\pi} V^{\\pi}$\n",
    "\n",
    "$Q^{\\pi} = T^{\\pi} Q^{\\pi}$\n",
    "\n",
    "$V^{*} = T^{*} V^{*}$\n",
    "\n",
    "$Q^{*} = T^{*} Q^{*}$\n",
    "\n",
    "### Properties of Bellman operators\n",
    "\n",
    "Bellman operators have several important properties. The properties that matters for us the most are:\n",
    "\n",
    "1. Monotonicity\n",
    "2. Contraction\n",
    "\n",
    "#### Monotonicity\n",
    "\n",
    "<img src='pngs/monotonicity.png'>\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have $V^{*}$ or $Q^{*}$, we can easily find $\\pi^{*}$\n",
    "\n",
    "For any $x \\in X$, the optimal policy is a greedy policy:\n",
    "\n",
    "$\\pi_g^{*} (x) = argmax_{a} Q^{*} (x, a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
