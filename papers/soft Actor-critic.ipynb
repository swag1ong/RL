{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Soft Actor Critic\n",
    "\n",
    "Model-free DRL algorithms  suffer from two major challenges:\n",
    "1. High sample complexity\n",
    "2. Brittle convergence properties, which necessitate meticulous hyper-parameter tuning.\n",
    "\n",
    "The authors introduce Soft actor-critic an offpolicy\n",
    "actor-critic deep RL algorithm based on the\n",
    "maximum entropy reinforcement learning framework. In this framework, the agent seeks to maximize the expected reward while also maximizing entropy. That is, to succeed at the task while acting\n",
    "as randomly as possible.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One cause for the poor sample efficiency of DRL methods is on-policy learning: TRPO, PPO, A3C require new samples to be collected for each gradient step. This quickly becomes extravagantly expensive, as the number\n",
    "of gradient steps and samples per step needed to learn\n",
    "an effective policy increases with task complexity. Off-policy can reuse past experience. This is not directly feasible with conventional policy gradient formulations, but is relatively straightforward for Q-learning based methods (DQN). However, the combination\n",
    "of off-policy learning and high-dimensional, nonlinear\n",
    "function approximation with neural networks presents a major\n",
    "challenge for stability and convergence.\n",
    "\n",
    "The algorithm is based on maximum entropy framework which augments the standard maximum reward objective with an entropy maximization term. The maximum entropy formulation provides a substantial improvement in exploration and robustness. The authors devise an off-policy maximum entropy actor-critic algorithm which provides beoth sample-efficient learning and stability.\n",
    "\n",
    "## Related work\n",
    "\n",
    "SAC algorithm incorporates three key ingredients:\n",
    "1. an actor-critic architecture with separate policy and value function networks\n",
    "2. an off-policy formulation that enables reuse of previously collected data for efficiency.\n",
    "3. entropy maximization to enable stability and exploration\n",
    "\n",
    "Actor-critic algorithms are typically derived\n",
    "starting from policy iteration, which alternates between policy\n",
    "evaluation—computing the value function for a policy—and policy improvement—using the value function to obtain\n",
    "a better policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}