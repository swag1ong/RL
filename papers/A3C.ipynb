{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In DQN, by storing the\n",
    "agent’s data in an experience replay memory, the data can\n",
    "be batched or randomly sampled from different time-steps to remove the correlation between samples and non-stationarity, but at the same time limits the methods to off-policy methods (it requires off-policy learning algorithms\n",
    "that can update from data generated by an older policy) and will have more memory, computation cost. In the paper, instead of experience replay, the authors asynchronously execute multiple agents in parallel, on-multiple instances of the environment. This parallelism also decorrelates the agents' data into a more stationary process, since at any given time-step, the parallel agents will be experiencing a variety of different states. This simple idea enables on-policy learning using Sarsa, n-step methods and actor-critic methods besides off-policy methods. At the same time, the algorithms do not rely on GPUs or massively distributed architectures.\n",
    "\n",
    "## Related Work\n",
    "\n",
    "The General Reinforcement Learning Architecture (Gorila) performs asynchronous training of reinforcement\n",
    "learning agents in a distributed setting. In Gorila,\n",
    "each process contains an actor that acts in its own copy\n",
    "of the environment, a separate replay memory, and a learner\n",
    "that samples data from the replay memory and computes\n",
    "gradients of the DQN loss with respect\n",
    "to the policy parameters. The gradients are asynchronously\n",
    "sent to a central parameter server which updates a central\n",
    "copy of the model. The updated policy parameters are sent\n",
    "to the actor-learners at fixed intervals.\n",
    "\n",
    "## Reinforcement Learning Background\n",
    "\n",
    "Let $Q(s, a; \\theta)$ be an approximate actoin-value function with parameters $\\theta$. In one-step Q-learning, the parameters $\\theta$ of the action value function $Q(s, a; \\theta)$ are learned by iteratively minimizing a sequence of loss functions (AVI), where the ith loss function defined as\n",
    "\n",
    "$L_i (\\theta_i) = E[r + \\gamma max_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}; \\theta_{i-1}) - Q(s, a; \\theta_i)]^2$\n",
    "\n",
    "One drawback of using one-step methods is that obtaining a reward r only directly affects the value of the state action pair s, a that led to the reward. The values of other state action paris are affected only indirectly through the updated value $Q(s, a)$. This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions. One way of propagating rewards faster is by using n-step returns. This results in a single reward r directly affecting the values of n preceding state action pairs. This makes the process of propagating rewards to relevant state-action paris potentially much more efficient (ie. some r is reused, less bias)\n",
    "\n",
    "In contrast to value-based methods, policy-based model-free\n",
    "methods directly parameterize the policy $\\pi(a | s; \\theta)$ and update the parameters $\\theta$ by performing, typically approximate, gradient ascent on $E[G_t]$. A learned estimate of the value function is commonly used\n",
    "as the baseline $b_t(s_t) \\approx V^{\\pi}(s_t)$ leading to a much lower variance estimate of the policy gradient. When an approximate value function is used as the baseline, the quantity $G_t - b_t$ used to scale the policy gradient can be seen as an estimate of the **advantage** of action $a_t$ in state $s_t$ or $A(a_t, s_t) = Q(a_t, s_t) - V(s_t)$. This approach can be viewed as an actor-critic architecture where the policy $\\pi$ is the actor and the baseline $b_t$ is the critic.\n",
    "\n",
    "## Asynchronous RL Framework\n",
    "\n",
    "While the underlying RL methods are quite different,\n",
    "with actor-critic being an on-policy policy search\n",
    "method and Q-learning being an off-policy value-based\n",
    "method, we use two main ideas to make all four algorithms\n",
    "practical given our design goal.\n",
    "\n",
    "First, we use asynchronous actor-learners on multiple CPU threads on a single machine. Keeping the lears on a single machine removes the communicaton costs of sending gradients and parameters and enables us to use Hogwild style updates for training.\n",
    "\n",
    "Second, we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment. Moreover, one can explicitly use different exploration polices in each actor-learner to maximize this diversity. By running different exploration policies in different threads, the overall changes being\n",
    "made to the parameters by multiple actor-learners applying\n",
    "online updates in parallel are likely to be less correlated\n",
    "in time than a single agent applying online updates. Hence, we do not use a replay memory and rely on parallel\n",
    "actors employing different exploration policies to perform\n",
    "the stabilizing role undertaken by experience replay in the\n",
    "DQN training algorithm.\n",
    "\n",
    "### Asynchronous one-step Q-learning (AVI, estimating $V^{*}$)\n",
    "\n",
    "Each thread interacts with its own copy of the environment and at each step computes a gradient of the Q-learning loss. they use a shared and slowly changing target network in computing the Q-learning loss as was proposed in the DQN training method. They also accumulate gradients over multiple timesteps before they are applied, which is similar using minibatches. This reduces the chances of multiple actor\n",
    "learners overwriting each other’s updates. Accumulating\n",
    "updates over several steps also provides some ability to\n",
    "trade off computational efficiency for data efficiency.\n",
    "\n",
    "Finally, they found that giving each thread a different exploration\n",
    "policy helps improve robustness. Adding diversity\n",
    "to exploration in this manner also generally improves performance\n",
    "through better exploration.\n",
    "\n",
    "<img src='pngs/algo_1.png'>\n",
    "\n",
    "### Asynchronous one-step SARSA (GPI):\n",
    "\n",
    "Most of the algorithm is the same as Asynchronous one-step Q-learning except that it uses different target value $r + \\gamma Q(s^{\\prime}, a^{\\prime};\\theta^-)$and follows GPI process.\n",
    "\n",
    "### Asynchronous n-step Q-learning\n",
    "\n",
    "The algorithm is somewhat unusual because\n",
    "it operates in the forward view by explicitly computing nstep\n",
    "returns, as opposed to the more common backward\n",
    "view used by techniques like eligibility traces. They found that using the forward view is easier\n",
    "when training neural networks with momentum-based\n",
    "methods and backpropagation through time.\n",
    "\n",
    "<img src='pngs/algo_3.png'>\n",
    "\n",
    "This process results in the agent\n",
    "receiving up to $t_{max}$ rewards from the environment since tis last update. The algorithm then computes gradients for n-step Q-learning updates for each of the state-action pairs encountered since the last update. Each n-step update uses the longest possible n-step return resulting in a one-step update for the last state, a two-step update for the second last state, and so on for a total of up to $t_max$ updates. The\n",
    "accumulated updates are applied in a single gradient step.\n",
    "### Asynchronous advantage actor-critic\n",
    "\n",
    "The A3C algorithm maintains a policy $\\pi(a_t|s_t;\\theta)$ and an estimate of the value function $V(s_t;\\theta_v)$. Like the variant of n-step Q-learning, the variant of actor-critic also operates in the forward view and uses the same mix of n-step returns to update both the policy and the value-function.\n",
    "The policy and the value function are updated after every $t_{max}$ actions or when a terminal state is reached. The update performed by the algorithm can be seen as $\\nabla_{\\theta^{\\prime}} log \\pi(a_t | s_t;\\theta^\\prime) A(s_t, a_t;\\theta, \\theta_v)$ where  $A(s_t, a_t;\\theta, \\theta_v) = \\sum_{i=0}^{k-1} \\gamma^{i} r_{t+i} + \\gamma^k V(s_{t+k}; \\theta_v) - V(s_t; \\theta_v)$ where k can vary from state to state and is upper-bounded by $t_{max}$.\n",
    "\n",
    "<img src='pngs/algo_4.png'>\n",
    "\n",
    "As with the value-based methods we rely on parallel actor-learners\n",
    "and accumulated updates for improving training\n",
    "stability. Note that while the parameters $\\theta$ of the policy and $\\theta_{v}$ of the value function are shown as being separate for generality, they always share some of the parameters in practice. The authors use a CNN that has one softmax output for the policy and one linear output for the value function with all non-output layers shared.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}