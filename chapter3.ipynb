{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finite Markov Decision Process\n",
    "\n",
    "## Agent-Environment interface\n",
    "\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from\n",
    "interaction to achieve a goal.\n",
    "\n",
    "1. Agent: the learner and decision maker.\n",
    "2. Environment: The thing that agent interacts with comprising everything outside of agent.\n",
    "3. Award: the special numerical values that the agent seeks to maximize over time through its choice of actions.\n",
    "\n",
    "The agent selects actions, the environment responding to these actions and presenting new situations to the agent.\n",
    "The environment also gives rise to rewards. In general, actions can be any decisions we want to learn how to make, and\n",
    "states can be anything we can know that might be useful in making them.\n",
    "\n",
    "<img src=\"agent-environment-interaction.png\">\n",
    "\n",
    "At each discrete time steps, t = 0, 1, 2, ..., the agent receives some representation of the environment's state $S_t \\in S$\n",
    "and on that basis selects an action, $A_t \\in A$. One time step later, in part as a consequence of its action $A_t$, the agent receives\n",
    "a numerical reward, $R_{t+1} \\in R$ and receives a new state $S_{t+1}$\n",
    "\n",
    "Thus, the MDP and agent\n",
    "together thereby give rise to a sequence or trajectory that begins like this:\n",
    "\n",
    "$S_0, A_0, S_{1}, R_{1}, A_{1}, ...$\n",
    "\n",
    "In finite MDP, $S, A(s), R$ are all finite, thus, we can associate $S, A(s), R$ with probability distributions.py\n",
    "\n",
    "$p(s^{\\prime}, r| s, a) = p(S_{t} = s^{\\prime}, R_t = r | S_{t-1} = s, A_{t-1} = a)$\n",
    "\n",
    "The function p defines the dynamics of the MDP means that the probability of current state $s^{\\prime}$ and reward r given previous state s and action a, so p has to satisfy:\n",
    "\n",
    "$\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, r| s, a) = 1$, for all $s \\in S, a \\in A(s)$. Since $\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, r| s, a) = \\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} \\frac{p(s^{\\prime}, s, a, r)}{p(s, a)}\n",
    "= \\frac{\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, s, a, r)}{p(s, a)} = \\frac{p(s, a)}{p(s, a)} = 1$\n",
    "\n",
    "At the same time, p satisfies the memory-less property (Markov property) which is assumed.\n",
    "\n",
    "From p, we can compute anything else one might need to know about the environment:\n",
    "\n",
    "1. probability of current state given previous state and action: $p(s^{\\prime} | s, a) = p(S_t = s^{\\prime} | S_{t-1} = s, A_{t-1} = a) = \\sum_{r \\in R} p(s^{\\prime}, r| s, a)$\n",
    "2. expected award for state action pairs: $r(a, s) = E(R_t | S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * p(R_t = r| S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * \\sum_{s^{\\prime} \\in S}p(R_t = r, S_t = s^{\\prime}| S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * \\sum_{s^{\\prime} \\in S}p(r, s^{\\prime}| s, a) $\n",
    "3. expected award for state action next state triples: $r(a, s, s^{\\prime}) = E(R_t = r | A_{t-1} = a, S_{t} = s^{\\prime}, S_{t-1} = s) = \\sum_{r \\in R_t} r * p(R_t = r| S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime}) = S_{t-1} = s) = \\sum_{r \\in R_t} r * \\frac{p(R_t = r, S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime})}{p(S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime}))} =\n",
    "\\sum_{r \\in R_t} r * \\frac{p(R_t = r, S_{t} = s^{\\prime} | S_{t-1}=s, A_{t-1} = a)}{p(S_{t} = s^{\\prime} |S_{t-1}=s, A_{t-1} = a)} = \\sum_{r \\in R_t} r * \\frac{p(r, s^{\\prime} | s, a)}{p(s^{\\prime} |s, a)}$\n",
    "\n",
    "## Goals and Rewards\n",
    "\n",
    "The primary goal of agent is to maximize the total amount of rewards it gets, we can formulate this informal idea into a reward hypothesis:\n",
    "**That all of what we mean by goals and purposes can be well thought of as\n",
    "the maximization of the expected value of the cumulative sum of a received\n",
    "scalar signal (called reward).**\n",
    "\n",
    "It is thus critical that the rewards we set up truly indicate what we want accomplished.\n",
    "In particular, the reward signal is not the place to impart to the agent prior knowledge\n",
    "about how to achieve what we want it to do.5 For example, a chess-playing agent should\n",
    "be rewarded only for actually winning, not for achieving subgoals such as taking its\n",
    "opponent’s pieces or aining control of the center of the board. If achieving these sorts\n",
    "of subgoals were rewarded, then the agent might find a way to achieve them without\n",
    "achieving the real goal. Better places for imparting this kind of prior knowledge are the initial policy or initial value function.\n",
    "\n",
    "## Returns and Episodes\n",
    "\n",
    "In general, we seek to maximize the expected return, where the return, denoted $G_t$, is defined as some specific function of the\n",
    "reward sequence. In the simplest case, the return is the sum of the rewards:\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + ... + R_{T}$\n",
    "\n",
    "### Episodic Tasks\n",
    "Where T is the final time step, it is a random variable, normally different for each subsequence. This approach makes sense when there is a final time step, that is, when the agent-environment interaction breaks naturally into\n",
    "subsequences, these subsequences are called *episodes*. Each episode ends in a special state called terminal state, followed by a reset to a standard\n",
    "starting state or to a sample from a standard distribution of starting states. Even if you think of episodes as\n",
    "ending in different ways, such as winning and losing a game, the next episode begins\n",
    "independently of how the previous one ended. Thus the episodes can all be considered to\n",
    "end in the same terminal state, with different rewards for the different outcomes. Tasks with episodes of this kind is called\n",
    "*episodic tasks*. In episodic tasks, we need to distinguish the set of all nonterminal states and terminal states $S^+$\n",
    "\n",
    "Examples: plays of a game,\n",
    "trips through a maze, or any sort of repeated interaction.\n",
    "\n",
    "### Continuing Tasks\n",
    "On the other hand, in many cases the agent–environment interaction does not break\n",
    "naturally into identifiable episodes, but goes on continually without limit, we call these tasks *continuing tasks*.\n",
    "The return formulation is problematic in this case becuase the final time step would be $T=\\infty$ which could also result $G_t = \\infty$.\n",
    "\n",
    "Examples: the natural way to formulate an on-going process-control task, or an\n",
    "application to a robot with a long life span.\n",
    "\n",
    "#### Discounted rewards\n",
    "\n",
    "According to continuing task approach,\n",
    "the agent tries to select actions so that the sum of the discounted rewards it receives over\n",
    "the future is maximized. In particular, it chooses $A_t$ to maximize the expected discounted return:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +... = \\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1}$\n",
    "\n",
    "Where $\\gamma \\in [0, 1]$ is a weight parameter called discount rate.\n",
    "\n",
    "The *discount rate* determines the present value of future rewards: a reward received k time steps in the future is worth only $\\gamma^{k-1}$ times what it would be\n",
    "worth if it were received immediately.\n",
    "1. If $\\gamma < 1$, $G_t$ has a finite value as long as the reward sequence $R_k$ is bounded.\n",
    "2. If $\\gamma = 0$, The agent is myopic in being only maximizing immediate rewards: its objective in this case is to learn how to choose $A_t$ so as to maximize only $R_{t+1}$.\n",
    "3. As $\\gamma \\rightarrow 1$, the return objective takes future rewards into account more strongly, the agent becomes more farsighted.\n",
    "\n",
    "Returns at successive time steps are related to each other in a way that is important for theory and algorithms of reinforcement learning:\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +... = R_{t+1} + \\gamma (R_{t+1+1} + \\gamma R_{t+1+2} + ...) = R_{t+1} + \\gamma G_{t+1}$\n",
    "\n",
    "## Unified Notation for Episodic and Continuing Tasks\n",
    "\n",
    "To unify two types of reinforcement learning tasks, we need to define the return as a sum over a finite number of terms in\n",
    "episodic tasks in one case and as an infinite number of terms in continuing tasks These two can be unified by considering episode termination to be\n",
    "the entering of a special absorbing state that transitions only to itself and that generates only rewards of zero.\n",
    "\n",
    "<img src=\"absorbing_state.png\">\n",
    "\n",
    "Here the solid square represents the special absorbing state corresponding to the end of an\n",
    "episode. Starting from S0, we get the reward sequence +1, +1, +1, 0, 0, 0, . . .. Summing\n",
    "these, we get the same return whether we sum over the first T rewards (here T = 3) or\n",
    "over the full infinite sequence. This remains true even if we introduce discounting. Thus,\n",
    "we can define the return, in general, including the possibility that $\\gamma = 1$ if the sum remains defined.\n",
    "Thus:\n",
    "\n",
    "$G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k = \\sum_{k=0}^{T} \\gamma^k R_{t+k+1}$\n",
    "\n",
    "Including the possibility that $T = \\infty$ or $\\gamma = 1$ (the sum is defined), but not both.\n",
    "\n",
    "## Policies and Value Functions\n",
    "\n",
    "Formally, a *policy* is a mapping from states to probabilities of selecting each possible action. If the agent is following policy $\\pi$\n",
    "at time t, then $\\pi(a|s)$ is the probability that $A_t = a$ given $S_t = s$ under policy $\\pi$. Reinforcement learning methods specify how\n",
    "the agent's policy is changed as a result of its experience.\n",
    "\n",
    "The *value function* is an estimation of how good it is for the agent to perform a given action in a given state. \"how good\" is defined in terms of\n",
    "future rewards that can be expected or expected return of being in a given state or performing a given action in a given state under a certain policy\n",
    "\n",
    "Formally, the state-value function of a state s under a policy $\\pi$, denoted $v_{\\pi} (s)$ is the expected return when **starting in s** and following $\\pi$ thereafter:\n",
    "\n",
    "$v_{\\pi} (s) = E_{\\pi}(G_t | S_t = s) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s], \\forall s \\in S$\n",
    "\n",
    "**The value of the terminal state, if any, is always zero, since starting in terminal state should give you zero reward**\n",
    "\n",
    "and $q_{\\pi} (s, a)$, The state-action value function for policy $\\pi$ is the expected return **starting from s, taking the action a**, and thereafter flowing policy $\\pi$:\n",
    "\n",
    "$q_{\\pi}(s, a) = E(G_t | S_t=s, A_t=a) = E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a]$\n",
    "\n",
    "We can also rewrite $v_{\\pi} (s)$ in terms of $q_{\\pi}(s, a), \\pi(a|s)$\n",
    "\n",
    "$v_{\\pi}(s) = E_{\\pi}(G_t | S_t = s) = \\sum_{g \\in G_t} g * p(G_t = g|S_t = s) = \\sum_{g\\in G_t} g * \\frac{p(G_t = g, S_t = s)}{p(s)} = \\sum_{g\\in G_t} g * \\sum_{a} \\frac{p(G_t = g, S_t = s, A_t=a)}{p(s)}$\n",
    "\n",
    "$ = \\sum_{g\\in G_t} g * \\sum_{a} \\frac{p(G_t = g | S_t = s, A_t=a)p(S_t=s, A_t=a)}{p(s)} = \\sum_{g\\in G_t} g * \\sum_{a} \\frac{p(G_t = g | S_t = s, A_t=a)p(A_t=a | S_t = s) p(s)}{p(s)}$\n",
    "\n",
    "$ = \\sum_{a} p(A_t=a | S_t = s) \\sum_{g\\in G_t} g *  p_{\\pi}(G_t = g | S_t = s, A_t=a)$\n",
    "\n",
    "$ = \\sum_{a} \\pi(a, s) * q_{\\pi}(a, s)$\n",
    "\n",
    "The value functions can be estimated from experience. For example, if an agent follows policy $\\pi$ and maintains an average, for each state encountered, of the actual returns that have followed that state,\n",
    "then the average will converge to the state's value $v_{\\pi}(s)$ as the number of times that state is encountered approaches infinity. If separate averages are kept for each\n",
    "action takeen in each state, then these averages will similarly converge to the action values, $q_{\\pi}(s, a)$\n",
    "\n",
    "### Recursive property and Bellman Equation\n",
    "\n",
    "A fundamental property of value functions used throughout reinforcement learning and\n",
    "dynamic programming is that they satisfy recursive relationships similar to that which\n",
    "we have already established for the return. For any policy $\\pi$, and any state s, the following consistency condition holds between the value of s and the vlaue of its possible\n",
    "successor states:\n",
    "\n",
    "\\begin{aligned}\n",
    "v_{\\pi} (s) & = E_{\\pi}[G_t | S_t = s]\\\\\n",
    "& = E_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t = s]\\\\\n",
    "& = E_{\\pi}[R_{t+1} | S_t=s] + \\gamma E_{\\pi}[G_{t+1} | S_t=s]\\\\\n",
    "E_{\\pi}[R_{t+1} | S_t=s] & = \\sum_{r} r * p(R_{t+1}=r | S_t=s)\\\\\n",
    "& = \\sum_{r} r * \\frac{p(R_{t+1}=r, S_t=s)}{p(S_t=s)}\\\\\n",
    "& = \\sum_{r} r * \\frac{\\sum_{s^{\\prime}} \\sum_{a} p(R_{t+1}=r, S_t=s, A_t = a, S_{t+1} = s^\\prime)}{p(S_t=s)}\\\\\n",
    "& = \\sum_{r} r * \\frac{\\sum_{s^{\\prime}} \\sum_{a} p(R_{t+1}=r, S_{t+1} = s^\\prime | S_t=s, A_t = a) p(A_t = a | S_t=s) p(S_t=s)}{p(S_t=s)}\\\\\n",
    "& = \\sum_{a}\\pi(a | s) \\sum_{r} r * \\sum_{s^{\\prime}} p(r, s^\\prime | s, a)\\\\\n",
    "E_{\\pi}[G_{t+1} | S_t=s] &= \\sum_{g^\\prime} g^\\prime * p(G_{t+1}=g^\\prime | S_t=s)\\\\\n",
    "&= \\sum_{g^\\prime} g^\\prime * \\frac{p(G_{t+1}=g^\\prime, S_t=s)}{p(S_t=s)}\\\\\n",
    "&= \\sum_{g^\\prime} g^\\prime * \\frac{\\sum_{s^{\\prime}} \\sum_{a} \\sum_{r} p(G_{t+1}=g^\\prime, S_t=s, R_{t+1}=r, S_{t+1} = s^{\\prime}, A_t = a)}{p(S_t=s)}\\\\\n",
    "&= \\sum_{g^\\prime} g^\\prime * \\frac{\\sum_{s^{\\prime}} \\sum_{a} \\sum_{r} p(G_{t+1}=g^\\prime | S_t=s, R_{t+1}=r, S_{t+1} = s^{\\prime}, A_t = a)p(S_t=s, R_{t+1}=r, S_{t+1} = s^{\\prime}, A_t = a)}{p(S_t=s)}\\\\\n",
    "&= \\sum_{g^\\prime} g^\\prime * \\frac{\\sum_{s^{\\prime}} \\sum_{a} \\sum_{r} \\underbrace{p(G_{t+1}=g^\\prime | S_t=s, R_{t+1}=r, S_{t+1} = s^{\\prime}, A_t = a)}_{G_{t+1} \\text{ only depend on } S_{t+1}}p(S_t=s, R_{t+1}=r, S_{t+1} = s^{\\prime}, A_t = a)}{p(S_t=s)}\\\\\n",
    "&= \\sum_{g^\\prime} g^\\prime * \\frac{\\sum_{s^{\\prime}} \\sum_{a} \\sum_{r} p(G_{t+1}=g^\\prime | S_{t+1} = s^{\\prime})p(R_{t+1}=r, S_{t+1} = s^{\\prime} | A_t = a, S_t=s)p(A_t=a | S_t=s)p(S_t=s)}{p(S_t=s)}\\\\\n",
    "&= \\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}}\\sum_{r}p(r, s^{\\prime} | a, s)\\sum_{g^\\prime} g^\\prime * p(G_{t+1}=g^\\prime | S_{t+1} = s^{\\prime})\\\\\n",
    "&= \\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}}\\sum_{r}p(r, s^{\\prime} | a, s) E_{\\pi}[G_{t+1}=g^\\prime | S_{t+1} = s^{\\prime}]\\\\\n",
    "\\implies v_{\\pi} (s) & = \\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}}\\sum_{r}p(r, s^{\\prime} | a, s)[r + \\gamma E_{\\pi}[G_{t+1}=g^\\prime | S_{t+1} = s^{\\prime}]]\\\\\n",
    "& = \\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}}\\sum_{r}p(r, s^{\\prime} | a, s)[r + \\gamma v_{\\pi}(s^\\prime)], \\forall s\\in S\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "So $v_{\\pi}$ can be easily read as the sum over all possible values of $a, s, s^\\prime$, for each triple, we compute the probability\n",
    "$\\pi(a | s) * p(r, s^{\\prime} | a, s)$ which is probability of action s given state s under policy $\\pi$ times the probability of next reward r and next state $s^\\prime$ given current state s and current action a.\n",
    "This probability then is multiplied by the sum of next reward plus discounted expected reward starting from next state\n",
    "\n",
    "$v_{\\pi}(s)$ is a unique solution to bellman equation. It expresses\n",
    "a relationship between the value of a state and the values of\n",
    "its successor states. It states that the value of the start state must equal the\n",
    "(discounted) value of the expected next state, plus the reward expected along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}