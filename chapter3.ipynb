{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Finite Markov Decision Process\n",
    "\n",
    "## Agent-Environment interface\n",
    "\n",
    "MDPs are meant to be a straightforward framing of the problem of learning from\n",
    "interaction to achieve a goal.\n",
    "\n",
    "1. Agent: the learner and decision maker.\n",
    "2. Environment: The thing that agent interacts with comprising everything outside of agent.\n",
    "3. Award: the special numerical values that the agent seeks to maximize over time through its choice of actions.\n",
    "\n",
    "The agent selects actions, the environment responding to these actions and presenting new situations to the agent.\n",
    "The environment also gives rise to rewards. In general, actions can be any decisions we want to learn how to make, and\n",
    "states can be anything we can know that might be useful in making them.\n",
    "\n",
    "<img src=\"agent-environment-interaction.png\">\n",
    "\n",
    "At each discrete time steps, t = 0, 1, 2, ..., the agent receives some representation of the environment's state $S_t \\in S$\n",
    "and on that basis selects an action, $A_t \\in A$. One time step later, in part as a consequence of its action $A_t$, the agent receives\n",
    "a numerical reward, $R_{t+1} \\in R$ and receives a new state $S_{t+1}$\n",
    "\n",
    "Thus, the MDP and agent\n",
    "together thereby give rise to a sequence or trajectory that begins like this:\n",
    "\n",
    "$S_0, A_0, S_{1}, R_{1}, A_{1}, ...$\n",
    "\n",
    "In finite MDP, $S, A(s), R$ are all finite, thus, we can associate $S, A(s), R$ with probability distributions.py\n",
    "\n",
    "$p(s^{\\prime}, r| s, a) = p(S_{t} = s^{\\prime}, R_t = r | S_{t-1} = s, A_{t-1} = a)$\n",
    "\n",
    "The function p defines the dynamics of the MDP means that the probability of current state $s^{\\prime}$ and reward r given previous state s and action a, so p has to satisfy:\n",
    "\n",
    "$\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, r| s, a) = 1$, for all $s \\in S, a \\in A(s)$. Since $\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, r| s, a) = \\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} \\frac{p(s^{\\prime}, s, a, r)}{p(s, a)}\n",
    "= \\frac{\\sum_{s^{\\prime} \\in S} \\sum_{r \\in R} p(s^{\\prime}, s, a, r)}{p(s, a)} = \\frac{p(s, a)}{p(s, a)} = 1$\n",
    "\n",
    "At the same time, p satisfies the memory-less property (Markov property) which is assumed.\n",
    "\n",
    "From p, we can compute anything else one might need to know about the environment:\n",
    "\n",
    "1. probability of current state given previous state and action: $p(s^{\\prime} | s, a) = p(S_t = s^{\\prime} | S_{t-1} = s, A_{t-1} = a) = \\sum_{r \\in R} p(s^{\\prime}, r| s, a)$\n",
    "2. expected award for state action pairs: $r(a, s) = E(R_t | S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * p(R_t = r| S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * \\sum_{s^{\\prime} \\in S}p(R_t = r, S_t = s^{\\prime}| S_{t-1}=s, A_{t-1} = a) = \\sum_{r \\in R_t} r * \\sum_{s^{\\prime} \\in S}p(r, s^{\\prime}| s, a) $\n",
    "3. expected award for state action next state triples: $r(a, s, s^{\\prime}) = E(R_t = r | A_{t-1} = a, S_{t} = s^{\\prime}, S_{t-1} = s) = \\sum_{r \\in R_t} r * p(R_t = r| S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime}) = S_{t-1} = s) = \\sum_{r \\in R_t} r * \\frac{p(R_t = r, S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime})}{p(S_{t-1}=s, A_{t-1} = a, S_{t} = s^{\\prime}))} =\n",
    "\\sum_{r \\in R_t} r * \\frac{p(R_t = r, S_{t} = s^{\\prime} | S_{t-1}=s, A_{t-1} = a)}{p(S_{t} = s^{\\prime} |S_{t-1}=s, A_{t-1} = a)} = \\sum_{r \\in R_t} r * \\frac{p(r, s^{\\prime} | s, a)}{p(s^{\\prime} |s, a)}$\n",
    "\n",
    "## Goals and Rewards\n",
    "\n",
    "The primary goal of agent is to maximize the total amount of rewards it gets, we can formulate this informal idea into a reward hypothesis:\n",
    "**That all of what we mean by goals and purposes can be well thought of as\n",
    "the maximization of the expected value of the cumulative sum of a received\n",
    "scalar signal (called reward).**\n",
    "\n",
    "It is thus critical that the rewards we set up truly indicate what we want accomplished.\n",
    "In particular, the reward signal is not the place to impart to the agent prior knowledge\n",
    "about how to achieve what we want it to do.5 For example, a chess-playing agent should\n",
    "be rewarded only for actually winning, not for achieving subgoals such as taking its\n",
    "opponentâ€™s pieces or aining control of the center of the board. If achieving these sorts\n",
    "of subgoals were rewarded, then the agent might find a way to achieve them without\n",
    "achieving the real goal. Better places for imparting this kind of prior knowledge are the initial policy or initial value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}