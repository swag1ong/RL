{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL\n",
    "\n",
    "\n",
    "## TicTacToe\n",
    "\n",
    "### Set up\n",
    "\n",
    "1. First we would set up a table of numbers, one for each possible state\n",
    "of the game. Each number will be the latest estimate of the probability of our winning\n",
    "from that state. We treat this estimate as the state’s value, and the whole table is the\n",
    "learned value function. State A has higher value than state B, or is considered “better”\n",
    "than state B, if the current estimate of the probability of our winning from A is higher\n",
    "than it is from B. (ie. for each reachable state, compute $p(winning | S_{t} = s)$) We set the initial values of all the other states to 0.5, representing a\n",
    "guess that we have a 50% chance of winning. (unknown values)\n",
    "\n",
    "2. We then play many games against the opponent. To select our moves we examine the\n",
    "states that would result from each of our possible moves (one for each blank space on the\n",
    "board) and look up their current values in the table. Most of the time we move greedily,\n",
    "selecting the move that leads to the state with greatest value, that is, with the highest\n",
    "estimated probability of winning (exploiting). Occasionally, however, we select randomly from among\n",
    "the other moves instead. These are called exploratory moves because they cause us to\n",
    "experience states that we might otherwise never see.\n",
    "\n",
    "3. While we are playing, we change the values of the states in which we find ourselves\n",
    "during the game. We attempt to make them more accurate estimates of the probabilities\n",
    "of winning. More precisely, the\n",
    "current value of the earlier state is updated to be closer to the value of the later state.\n",
    "This can be done by moving the earlier state’s value a fraction of the way toward the\n",
    "value of the later state. Exploratory moves do not result in any learning,\n",
    "but each of our other moves does.\n",
    "\n",
    " - Let $S$ be the state before a move, $S^\\prime$ be the state after the move, then $V(S) = V(S) + \\alpha [V(S^\\prime) - V(S)]$. This is a temporal-difference\n",
    " learning method, where $\\alpha$ is a learning rate\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}